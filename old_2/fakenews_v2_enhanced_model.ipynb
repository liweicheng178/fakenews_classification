{"cells": [{"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "3.6.10\nCPU times: user 56.2 ms, sys: 8.48 ms, total: 64.7 ms\nWall time: 62.3 ms\n"}], "source": "%%time\nimport pandas as pd\nimport os, sys, time, json, re, string, datetime\n\nfrom pyspark import SparkContext, SparkConf, StorageLevel, keyword_only\nfrom pyspark.sql.types import IntegerType\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.ml.linalg import Vectors, VectorUDT\nfrom pyspark.ml.param.shared import HasInputCol, HasInputCols, HasOutputCol, HasOutputCols, Param\nfrom pyspark.ml.feature import OneHotEncoder, HashingTF, IDF, Tokenizer, RegexTokenizer, NGram, CountVectorizer\nfrom pyspark.ml.feature import StopWordsRemover, VectorAssembler, PCA, OneHotEncoderEstimator,StringIndexer\n\nfrom pyspark.ml.classification import LogisticRegression, NaiveBayes, DecisionTreeClassifier, RandomForestClassifier\nfrom pyspark.ml.classification import GBTClassifier, MultilayerPerceptronClassifier\n\nfrom pyspark.ml import Pipeline, Transformer\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom platform import python_version\nprint(python_version())"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 3 \u00b5s, sys: 1 \u00b5s, total: 4 \u00b5s\nWall time: 6.68 \u00b5s\n"}, {"data": {"text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://cluster-b477-m.asia-southeast1-a.c.weicheng.internal:4042\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.5</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        ", "text/plain": "<SparkContext master=yarn appName=PySparkShell>"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "%%time\nsc"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 0 ns, sys: 7 ms, total: 7 ms\nWall time: 8.48 ms\n"}], "source": "%%time\nspark = SparkSession.builder \\\n        .appName(\"fakenews\") \\\n        .config(\"spark.master\", \"yarn\") \\\n        .config(\"spark.submit.deployMode\", \"cluster\") \\\n        .config(\"spark.driver.memory\", \"25g\") \\\n        .config(\"spark.executor.instances\", \"5\") \\\n        .config(\"spark.executor.cores\", \"4\") \\\n        .config(\"spark.executor.memory\", \"25g\") \\\n        .getOrCreate()"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 217806 entries, 0 to 108010\nData columns (total 7 columns):\ndomain             217806 non-null object\ntype               217806 non-null object\ncontent            217806 non-null object\ntitle              217806 non-null object\nauthors            217806 non-null object\nauthors_missing    217806 non-null object\nlabel              217806 non-null object\ndtypes: object(7)\nmemory usage: 13.3+ MB\nNone\nCPU times: user 8.84 s, sys: 2.12 s, total: 11 s\nWall time: 23 s\n"}, {"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>domain</th>\n      <th>type</th>\n      <th>content</th>\n      <th>title</th>\n      <th>authors</th>\n      <th>authors_missing</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>beforeitsnews.com</td>\n      <td>fake</td>\n      <td>Boehner presses to make tax cuts permanent\\n\\n...</td>\n      <td>Boehner presses to make tax cuts permanent</td>\n      <td>United Liberty</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>beforeitsnews.com</td>\n      <td>fake</td>\n      <td>An Armed Good Samaritan Who Doesn\u2019t Want The S...</td>\n      <td>An Armed Good Samaritan Who Doesn\u2019t Want The S...</td>\n      <td>The Real Revo</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beforeitsnews.com</td>\n      <td>fake</td>\n      <td>(Before It's News)\\n\\nby Rob Morphy\\n\\nLegends...</td>\n      <td>Village Of The Dead: The Anjikuni Mystery</td>\n      <td>Rob Morphy, Mort Amsel</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>teaparty.org</td>\n      <td>fake</td>\n      <td>(Breitbart) \u2013 With his hysterical gotcha attac...</td>\n      <td>Trump Calls Out Race-Baiting ABC News Reporter</td>\n      <td>nan</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>beforeitsnews.com</td>\n      <td>fake</td>\n      <td>Researchers develop new depression diagnosis a...</td>\n      <td>Researchers develop new depression diagnosis a...</td>\n      <td>Bel Marra Health</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>108006</th>\n      <td>sports.yahoo.com</td>\n      <td>reliable</td>\n      <td>View photos\\nMichigan guard Zak Irvin, right, ...</td>\n      <td>No. 25 Michigan beats Mount St. Mary's 64-47</td>\n      <td>The Associated Press</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>108007</th>\n      <td>uk.finance.yahoo.com</td>\n      <td>reliable</td>\n      <td>President-elect Donald Trump continued his scr...</td>\n      <td>Trump quotes Hillary Clinton, rages against Wi...</td>\n      <td>Maxwell Tani</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>108008</th>\n      <td>www.yahoo.com</td>\n      <td>reliable</td>\n      <td>Holiday shoppers eager to snag big discounts t...</td>\n      <td>Dialing up deals: Black Friday online sales hi...</td>\n      <td>ALEX VEIGA</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>108009</th>\n      <td>www.reuters.com</td>\n      <td>reliable</td>\n      <td>Kabul police raid shisha cafes in crackdown on...</td>\n      <td>Kabul police raid shisha cafes in crackdown on...</td>\n      <td>nan</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>108010</th>\n      <td>m.mlb.com</td>\n      <td>reliable</td>\n      <td>2016-17 MLB.com Hot Stove \\n\u00a9 2001- 2016 MLB A...</td>\n      <td>Possibility of Granderson in CF | MLB.com</td>\n      <td>nan</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>217806 rows \u00d7 7 columns</p>\n</div>", "text/plain": "                      domain      type  \\\n0          beforeitsnews.com      fake   \n1          beforeitsnews.com      fake   \n2          beforeitsnews.com      fake   \n3               teaparty.org      fake   \n4          beforeitsnews.com      fake   \n...                      ...       ...   \n108006      sports.yahoo.com  reliable   \n108007  uk.finance.yahoo.com  reliable   \n108008         www.yahoo.com  reliable   \n108009       www.reuters.com  reliable   \n108010             m.mlb.com  reliable   \n\n                                                  content  \\\n0       Boehner presses to make tax cuts permanent\\n\\n...   \n1       An Armed Good Samaritan Who Doesn\u2019t Want The S...   \n2       (Before It's News)\\n\\nby Rob Morphy\\n\\nLegends...   \n3       (Breitbart) \u2013 With his hysterical gotcha attac...   \n4       Researchers develop new depression diagnosis a...   \n...                                                   ...   \n108006  View photos\\nMichigan guard Zak Irvin, right, ...   \n108007  President-elect Donald Trump continued his scr...   \n108008  Holiday shoppers eager to snag big discounts t...   \n108009  Kabul police raid shisha cafes in crackdown on...   \n108010  2016-17 MLB.com Hot Stove \\n\u00a9 2001- 2016 MLB A...   \n\n                                                    title  \\\n0              Boehner presses to make tax cuts permanent   \n1       An Armed Good Samaritan Who Doesn\u2019t Want The S...   \n2               Village Of The Dead: The Anjikuni Mystery   \n3          Trump Calls Out Race-Baiting ABC News Reporter   \n4       Researchers develop new depression diagnosis a...   \n...                                                   ...   \n108006       No. 25 Michigan beats Mount St. Mary's 64-47   \n108007  Trump quotes Hillary Clinton, rages against Wi...   \n108008  Dialing up deals: Black Friday online sales hi...   \n108009  Kabul police raid shisha cafes in crackdown on...   \n108010          Possibility of Granderson in CF | MLB.com   \n\n                       authors authors_missing label  \n0               United Liberty               0     1  \n1                The Real Revo               0     1  \n2       Rob Morphy, Mort Amsel               0     1  \n3                          nan               1     1  \n4             Bel Marra Health               0     1  \n...                        ...             ...   ...  \n108006    The Associated Press               0     0  \n108007            Maxwell Tani               0     0  \n108008              ALEX VEIGA               0     0  \n108009                     nan               1     0  \n108010                     nan               1     0  \n\n[217806 rows x 7 columns]"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "%%time\ndf_fake_train = pd.read_csv('gs://dataproc-6ca41800-27b4-47d5-abee-55c011dfa389-asia-southeast1/data/fake-news/100k_fake_news_cleaned_dataset.csv')\ndf_fake_train[['authors_missing']] = df_fake_train[['authors_missing']].astype(int)\ndf_fake_train['label'] = 1\ndf_reliable_train = pd.read_csv('gs://dataproc-6ca41800-27b4-47d5-abee-55c011dfa389-asia-southeast1/data/fake-news/100k_reliable_news_cleaned_dataset.csv')\ndf_reliable_train[['authors_missing']] = df_reliable_train[['authors_missing']].astype(int)\ndf_reliable_train['label'] = 0\ndf_news_train = pd.concat([df_fake_train, df_reliable_train])\ndf_news_train[df_news_train.columns] = df_news_train[df_news_train.columns].astype(str)\nprint(df_news_train.info())\ndf_news_train"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+----+--------------------+--------------------+--------------------+---------------+-----+\n|           domain|type|             content|               title|             authors|authors_missing|label|\n+-----------------+----+--------------------+--------------------+--------------------+---------------+-----+\n|beforeitsnews.com|fake|Boehner presses t...|Boehner presses t...|      United Liberty|              0|    1|\n|beforeitsnews.com|fake|An Armed Good Sam...|An Armed Good Sam...|       The Real Revo|              0|    1|\n|beforeitsnews.com|fake|(Before It's News...|Village Of The De...|Rob Morphy, Mort ...|              0|    1|\n|     teaparty.org|fake|(Breitbart) \u2013 Wit...|Trump Calls Out R...|                 nan|              1|    1|\n|beforeitsnews.com|fake|Researchers devel...|Researchers devel...|    Bel Marra Health|              0|    1|\n|beforeitsnews.com|fake|Trading Watch Lis...|Trading Watch Lis...|Bulls On Wall Street|              0|    1|\n|beforeitsnews.com|fake|Looks like today ...|Looks like today ...|      Protein Wisdom|              0|    1|\n|beforeitsnews.com|fake|On the paradox of...|On the paradox of...|The Adam Smith In...|              0|    1|\n|beforeitsnews.com|fake|% of readers thin...|Minuteman- Most I...|         Nc Renegade|              0|    1|\n|beforeitsnews.com|fake|Juniper to Trim E...|Juniper to Trim E...|Zacks Investment ...|              0|    1|\n+-----------------+----+--------------------+--------------------+--------------------+---------------+-----+\nonly showing top 10 rows\n\nCPU times: user 13.4 s, sys: 2.32 s, total: 15.7 s\nWall time: 22.3 s\n"}], "source": "%%time\ndf_news = spark.createDataFrame(df_news_train)\ndf_news = df_news.withColumn(\"label\", df_news[\"label\"].cast(IntegerType()))\ndf_news = df_news.withColumn(\"authors_missing\", df_news[\"authors_missing\"].cast(IntegerType()))\ndf_news.show(10)"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 5.17 ms, sys: 1.01 ms, total: 6.18 ms\nWall time: 404 ms\n"}], "source": "%%time\n#remove empty content which will cause problem when transform the text\ndf_news = df_news.filter(df_news.content != \"\")\ndf_news = df_news.filter(df_news.title != \"\")\ndf_news = df_news.dropDuplicates(['label', 'content', 'title', 'authors_missing'])"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": "# only keep type and content\ndf_news = df_news.select('label', 'content', 'title','authors_missing')\n\n# split the dataset\ndf_train, df_test = df_news.randomSplit([0.8, 0.2], seed=666)\nparam_tuning = False"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 94 \u00b5s, sys: 18 \u00b5s, total: 112 \u00b5s\nWall time: 119 \u00b5s\n"}], "source": "%%time\n# customized transformer class to manually extract some counting based text features\nclass ReviewContentTransformer(Transformer, HasInputCol, HasOutputCol):\n\n    @keyword_only\n    def __init__(self, inputCol=\"content\", outputCol=\"content_features\"):\n        super(ReviewContentTransformer, self).__init__()\n        kwargs = self._input_kwargs\n        self.setParams(**kwargs)\n\n    @keyword_only\n    def setParams(self, inputCol=None, outputCol=None):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n\n    def _transform(self, dataset):\n        \n        def f(s):\n            uppercase_count = 0\n            char_count = 0\n            for c in s:                \n                if c in string.ascii_uppercase:\n                    uppercase_count += 1\n                    char_count += 1\n                elif c in string.ascii_lowercase:\n                    char_count += 1\n            \n            text_len = len(s)\n            return Vectors.dense(text_len, char_count, \n                                 uppercase_count, uppercase_count / (char_count + 1e-10))\n\n        return dataset.withColumn(self.getOutputCol(), \n                                  F.udf(f, VectorUDT())(dataset[self.getInputCol()]))"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 100 \u00b5s, sys: 20 \u00b5s, total: 120 \u00b5s\nWall time: 126 \u00b5s\n"}], "source": "%%time\n# customized transformer class to manually extract some counting based word features\nclass ReviewWordsTransformer(Transformer, HasInputCol, HasOutputCol):\n\n    @keyword_only\n    def __init__(self, inputCol=\"content\", outputCol=\"content_features\"):\n        super(ReviewWordsTransformer, self).__init__()\n        kwargs = self._input_kwargs\n        self.setParams(**kwargs)\n\n    @keyword_only\n    def setParams(self, inputCol=None, outputCol=None):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n\n    def _transform(self, dataset):\n        \n        def f(words):    \n            word_count = len(words)\n            unique_word_count = len(set(words))\n            upper_words = []\n            for w in words:\n                if w.isupper():\n                    upper_words.append(w)\n            upper_word_count = len(set(upper_words))\n            unique_upper_word_count = len(upper_words)\n            return Vectors.dense(word_count, unique_word_count, unique_word_count / (word_count + 1e-10),\n                                 upper_word_count, upper_word_count / (word_count + 1e-10), \n                                 unique_upper_word_count, unique_upper_word_count / (upper_word_count + 1e-10))\n\n        return dataset.withColumn(self.getOutputCol(), \n                                  F.udf(f, VectorUDT())(dataset[self.getInputCol()]))"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 12 \u00b5s, sys: 2 \u00b5s, total: 14 \u00b5s\nWall time: 18.6 \u00b5s\n"}], "source": "%%time\n# show model prediction performance on the given dataset\ndef eval_model_perf(fitted_model, dataset, label_col=\"label\", prediction_col=\"prediction\", probability_col=\"probability\"):\n    pred_dataset = fitted_model.transform(dataset)\n    eval_dataset = pred_dataset.select(label_col, prediction_col, probability_col)\n    # model performance evaluation\n    metricNames = [\"accuracy\", \"f1\"]\n    model_eval = MulticlassClassificationEvaluator(predictionCol=prediction_col, labelCol=label_col)\n    for m in metricNames:\n        val = model_eval.evaluate(eval_dataset, {model_eval.metricName: m})\n        print(m, \" = \", val)\n    roc_eval = BinaryClassificationEvaluator(rawPredictionCol=probability_col, labelCol=label_col, metricName=\"areaUnderROC\")\n    print(\"AUC =\", roc_eval.evaluate(eval_dataset))    \n    return pred_dataset\n\n# show CV param tunning result\ndef show_cv_results(cv_model):\n    for result, param in sorted(zip(cv_model.avgMetrics, cv_model.getEstimatorParamMaps()), reverse=True, key=lambda x: x[0]):\n        print(result, \" | \", param)"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 6 \u00b5s, sys: 1 \u00b5s, total: 7 \u00b5s\nWall time: 11.9 \u00b5s\n"}], "source": "%%time\ndef run_models(df_train, df_test, without_pca = False):\n    print(\"**********LogisticRegression**********\")\n    t = time.time()\n    lr_model = LogisticRegression(featuresCol='features', \n                                  labelCol='label', \n                                  predictionCol='prediction', \n                                  probabilityCol='probability', \n                                  rawPredictionCol='rawPrediction',\n                                  family='binomial', \n                                  fitIntercept=True, \n                                  threshold=0.5, \n                                  standardization=False, \n                                  maxIter=200, \n                                  regParam=0.005, \n                                  elasticNetParam=0, \n                                  tol=1e-06, \n                                  aggregationDepth=2)\n\n    lr_model = lr_model.fit(df_train)\n    \n    eval_model_perf(lr_model, df_test)\n    \n    print(\"time taken for LogisticRegression: \", time.time() - t)\n    t = time.time()\n\n    print(\"**********DecisionTreeClassifier**********\")\n    dt_model = DecisionTreeClassifier(featuresCol='features', \n                                      labelCol='label', \n                                      predictionCol='prediction', \n                                      probabilityCol='probability', \n                                      rawPredictionCol='rawPrediction', \n                                      maxDepth=10, maxBins=32, \n                                      minInstancesPerNode=1, \n                                      minInfoGain=0.0, \n                                      maxMemoryInMB=2048, \n                                      cacheNodeIds=True, \n                                      checkpointInterval=10,\n                                      impurity='gini', \n                                      seed=666)\n\n    dt_model = dt_model.fit(df_train)\n    eval_model_perf(dt_model, df_test)\n    print(\"time taken for DecisionTreeClassifier: \", time.time() - t)\n    t = time.time()\n    \n    print(\"**********RandomForestClassifier**********\")\n    rf_model = RandomForestClassifier(featuresCol='features', \n                                      labelCol='label', \n                                      predictionCol='prediction', \n                                      probabilityCol='probability', \n                                      rawPredictionCol='rawPrediction',\n                                      maxDepth=10, \n                                      maxBins=32, \n                                      minInstancesPerNode=1, \n                                      minInfoGain=0.0, \n                                      maxMemoryInMB=2048, \n                                      cacheNodeIds=True, \n                                      checkpointInterval=10, \n                                      impurity='gini', \n                                      numTrees=200, \n                                      featureSubsetStrategy='auto', \n                                      seed=666, \n                                      subsamplingRate=0.8)\n\n    rf_model = rf_model.fit(df_train)\n    eval_model_perf(rf_model, df_test)\n    print(\"time taken for RandomForestClassifier: \", time.time() - t)\n    t = time.time()\n    \n    print(\"**********GBTClassifier**********\")\n    gbt_model = GBTClassifier(featuresCol='features', \n                             labelCol='label', \n                             maxIter=250)\n\n    gbt_model = gbt_model.fit(df_train)\n    eval_model_perf(gbt_model, df_test)\n    print(\"time taken for GBTClassifier: \", time.time() - t)\n    t = time.time()    \n    \n    print(\"**********MultilayerPerceptronClassifier**********\")\n    mp_model = MultilayerPerceptronClassifier(featuresCol='features', \n                                              labelCol='label', \n                                              predictionCol='prediction', \n                                              layers=[4, 5, 4, 3],  \n                                              maxIter=100, \n                                              blockSize=128, \n                                              seed=1234)\n\n    mp_model = mp_model.fit(df_train)\n    eval_model_perf(mp_model, df_test)\n    print(\"time taken for MultilayerPerceptronClassifier: \", time.time() - t)\n    t = time.time() \n    \n    if without_pca:\n        print(\"**********NaiveBayes**********\")\n        nb_model = NaiveBayes(featuresCol='features', \n                              labelCol='label', \n                              predictionCol='prediction', \n                              probabilityCol='probability', \n                              rawPredictionCol='rawPrediction', \n                              smoothing=1, \n                              modelType='multinomial')\n\n        nb_model = mp_model.fit(df_train)\n        eval_model_perf(nb_model, df_test)\n        print(\"time taken for NaiveBayes: \", time.time() - t)    "}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 5 \u00b5s, sys: 1 \u00b5s, total: 6 \u00b5s\nWall time: 11 \u00b5s\n"}], "source": "%%time\ndef build_data_preproc_model_with_pca(vocab_size=5000):\n    preproc_steps = [\n        RegexTokenizer(inputCol=\"content\", outputCol=\"all_words\", pattern=r\"\\W\"),\n        StopWordsRemover(inputCol=\"all_words\", outputCol=\"words\"),\n        CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=vocab_size),\n        IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\"),\n        PCA(inputCol=\"tfidf_features\", outputCol=\"pca_features\", k=100),\n        \n        ReviewContentTransformer(inputCol=\"content\", outputCol=\"content_features\"),\n        ReviewWordsTransformer(inputCol=\"words\", outputCol=\"word_features\"),\n        \n        RegexTokenizer(inputCol=\"title\", outputCol=\"all_title_words\", pattern=r\"\\W\"),\n        StopWordsRemover(inputCol=\"all_title_words\", outputCol=\"title_words\"),\n        CountVectorizer(inputCol=\"title_words\", outputCol=\"title_tf_features\", vocabSize=100),\n        IDF(inputCol=\"title_tf_features\", outputCol=\"title_tfidf_features\"),\n        PCA(inputCol=\"title_tfidf_features\", outputCol=\"title_pca_features\", k=100),   \n        \n        StringIndexer(inputCol=\"authors_missing\", outputCol=\"authors_missing_indexed\", handleInvalid='keep'),\n        OneHotEncoder(inputCol=\"authors_missing_indexed\", outputCol=\"authors_missing_feature\"),\n        \n        VectorAssembler(inputCols=[\"pca_features\", \"title_pca_features\", \"title_tfidf_features\", \n                                   \"content_features\", \"word_features\", \"authors_missing_feature\"], \n                        outputCol=\"features\")\n    ]\n    return Pipeline(stages=preproc_steps)\n\ndef build_data_preproc_model_without_pca(vocab_size=5000):\n    preproc_steps = [\n        RegexTokenizer(inputCol=\"content\", outputCol=\"all_words\", pattern=r\"\\W\"),\n        StopWordsRemover(inputCol=\"all_words\", outputCol=\"words\"),\n        \n        StringIndexer(inputCol=\"authors_missing\", outputCol=\"authors_missing_indexed\", handleInvalid='keep'),\n        OneHotEncoder(inputCol=\"authors_missing_indexed\", outputCol=\"authors_missing_feature\"),\n        \n        ReviewContentTransformer(inputCol=\"content\", outputCol=\"content_features\"),\n        ReviewWordsTransformer(inputCol=\"words\", outputCol=\"word_features\"),\n        \n        VectorAssembler(inputCols=[\"content_features\", \"word_features\", \"authors_missing_feature\"], outputCol=\"features\")\n    ]\n    return Pipeline(stages=preproc_steps)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "**********Run Models with PCA Features**********\n[Row(label=0, features=DenseVector([-18.8241, -3.4875, 1.7624, 4.4359, 3.5469, -5.2796, 1.0957, 1.7793, -5.1523, 2.0355, 0.7305, -4.1008, 1.4991, -2.0755, 1.9944, -4.8548, 1.0588, 5.061, -3.2008, -4.7146, 12.072, 4.5596, 0.6113, 2.2505, 1.8553, 3.8479, -6.1946, 2.9211, -8.9351, 1.606, -2.5601, -2.2386, -15.2543, 8.6931, 3.5452, -4.6838, 3.0176, 9.5615, -1.9669, -2.1753, -4.7353, 0.2118, -5.7341, -5.3679, -6.0063, 3.9125, 1.801, 9.1861, 2.8239, 1.2538, -1.4186, 7.1026, -1.094, -6.084, -1.8551, 5.504, 1.0316, -3.4852, -2.3373, -0.5905, -2.482, 1.5996, -8.4547, -1.1193, -5.0701, 7.5682, 4.9765, -1.8888, -0.4902, -5.4125, -0.1729, 1.7335, 4.8978, 2.9605, 3.0806, -2.9867, -4.0572, -6.3073, 3.4909, 6.2352, -1.0364, -1.0758, 3.2993, -0.6111, -2.1284, 3.8133, 4.0526, 5.2001, -5.7574, 0.2304, -0.2902, -0.5034, 6.2816, 3.214, 2.1842, -0.1246, -1.4028, -7.7809, -6.0749, -5.2691, -0.0254, 0.015, 0.0258, -0.0004, -0.0263, -0.0064, 0.028, -0.0451, 0.0339, 0.0256, -0.0202, 0.0109, 0.0817, -0.0944, 0.0296, 0.0229, -0.0625, -0.008, 0.0931, 0.054, 0.0125, -0.0759, -0.0746, -0.0443, -0.0287, -0.0836, -0.0827, -0.007, 0.0638, -0.0704, 0.0017, -0.214, 0.0421, 0.1484, 0.108, 0.0345, -0.1128, -0.0449, 0.1192, 0.127, -0.0193, 0.1545, 0.1614, 0.2107, -0.1062, -0.1038, 0.0758, 0.4466, 0.0156, -0.1538, 0.2159, -0.0409, 0.2015, 0.0302, -0.0985, 0.3082, 0.1974, 0.465, -0.5925, -1.0407, -0.6277, 3.3155, 0.8962, -0.5952, 0.9421, -0.2218, 0.8431, -0.3242, 0.278, -0.1427, 0.3832, 0.1409, 0.1705, -0.4164, -0.2991, -0.7337, 0.1477, -0.1187, 0.8316, -0.5818, 0.6081, -1.543, -0.5553, 0.3217, -0.5736, -1.2864, 0.7713, -0.2574, 0.088, 0.0652, -0.067, 0.2432, 0.0012, -0.0274, 0.0068, 0.0369, 0.0195, 0.0023, 0.0026, -0.0003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.9776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5741.0, 4510.0, 182.0, 0.0404, 549.0, 329.0, 0.5993, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]))]\n[Row(label=0, features=SparseVector(313, {0: -5.742, 1: -0.2963, 2: -1.5631, 3: 0.9896, 4: 0.0161, 5: -1.5343, 6: -0.8203, 7: -0.2657, 8: -0.9079, 9: 1.4554, 10: 0.7874, 11: -1.767, 12: 1.0369, 13: 0.4861, 14: -0.4247, 15: -2.975, 16: 0.1313, 17: -0.0692, 18: -0.3773, 19: 0.0205, 20: 0.71, 21: 2.1786, 22: 0.7482, 23: 2.7273, 24: 0.2166, 25: 0.8981, 26: 0.8976, 27: 0.0259, 28: -1.2533, 29: -3.136, 30: 1.0791, 31: 4.0402, 32: 0.5682, 33: 3.4143, 34: -1.9823, 35: -0.3201, 36: 1.4143, 37: 2.527, 38: 0.3666, 39: 3.0786, 40: 4.4292, 41: 2.9771, 42: -3.5576, 43: -0.5086, 44: -4.6681, 45: -1.5777, 46: -1.3037, 47: -2.7932, 48: 4.8385, 49: -3.6598, 50: -1.3157, 51: -0.3614, 52: 1.7376, 53: -1.412, 54: -2.0861, 55: -0.9596, 56: -0.0259, 57: 1.8485, 58: 0.9744, 59: 0.5608, 60: -0.966, 61: -0.6144, 62: -1.5953, 63: 0.6512, 64: 3.511, 65: -2.2601, 66: 0.2768, 67: -0.6296, 68: 0.2252, 69: 3.6169, 70: -3.8172, 71: 0.9922, 72: 1.3853, 73: -1.943, 74: -1.5149, 75: -0.9768, 76: 2.113, 77: -1.2403, 78: -1.9524, 79: 0.2643, 80: -1.71, 81: 3.3675, 82: 3.098, 83: -0.3809, 84: -1.1055, 85: 1.3985, 86: 3.8737, 87: 1.0742, 88: -2.6099, 89: 2.8602, 90: 2.5993, 91: 1.2784, 92: 1.5556, 93: -0.5201, 94: -4.3555, 95: -3.1908, 96: 1.0721, 97: 3.4574, 98: 3.023, 99: 0.6747, 300: 1862.0, 301: 1481.0, 302: 104.0, 303: 0.0702, 304: 182.0, 305: 146.0, 306: 0.8022, 312: 1.0}))]\n**********LogisticRegression**********\naccuracy  =  0.8900997414111562\nf1  =  0.8899885592389403\nAUC = 0.952914960989118\ntime taken for LogisticRegression:  0:04:23.345775\n**********DecisionTreeClassifier**********\naccuracy  =  0.8287079793128925\nf1  =  0.8286818269563927\nAUC = 0.8988270725746094\ntime taken for DecisionTreeClassifier:  0:04:21.389089\n**********RandomForestClassifier**********\naccuracy  =  0.865487624676764\nf1  =  0.8654546119989036\nAUC = 0.9414119749159164\ntime taken for RandomForestClassifier:  0:10:13.294548\n**********GBTClassifier**********\naccuracy  =  0.9042066863686739\nf1  =  0.9042048600001236\nAUC = 0.9683020455059095\ntime taken for GBTClassifier:  1:14:16.733835\n**********MultilayerPerceptronClassifier**********\naccuracy  =  0.865487624676764\nf1  =  0.8654546119989036\nAUC = 0.9414119749159171\ntime taken for MultilayerPerceptronClassifier:  0:04:17.148360\nCPU times: user 45.9 s, sys: 21.5 s, total: 1min 7s\nWall time: 1h 40min 17s\n"}], "source": "%%time\nprint(\"**********Run Models with PCA Features**********\")\n# generate the features to be used for model training\npreproc_model = build_data_preproc_model_with_pca(2000).fit(df_train)\ndf_train_pca = preproc_model.transform(df_train).select(\"label\", \"features\")\nprint(df_train_pca.take(1))\ndf_test_pca = preproc_model.transform(df_test).select(\"label\", \"features\")\nprint(df_test_pca.take(1))\nrun_models(df_train_pca, df_test_pca)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "**********Run Models without PCA Features**********\n[Row(label=0, features=DenseVector([5741.0, 4510.0, 182.0, 0.0404, 549.0, 329.0, 0.5993, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]))]\n[Row(label=0, features=DenseVector([1862.0, 1481.0, 104.0, 0.0702, 182.0, 146.0, 0.8022, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]))]\naccuracy  =  0.5702807536017732\nf1  =  0.5659001574227993\nAUC = 0.6287591998742492\nCPU times: user 389 ms, sys: 184 ms, total: 573 ms\nWall time: 2min 11s\n"}, {"data": {"text/plain": "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"}, "execution_count": 15, "metadata": {}, "output_type": "execute_result"}], "source": "%%time\nprint(\"**********Run Models without PCA Features**********\")\n# generate the features to be used for model training\npreproc_model = build_data_preproc_model_without_pca(3000).fit(df_train)\ndf_train_wo_pca = preproc_model.transform(df_train).select(\"label\", \"features\")\nprint(df_train_wo_pca.take(1))\ndf_test_wo_pca = preproc_model.transform(df_test).select(\"label\", \"features\")\nprint(df_test_wo_pca.take(1))\nrun_models(df_train_wo_pca, df_test_wo_pca, without_pca = True)"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.10"}}, "nbformat": 4, "nbformat_minor": 2}
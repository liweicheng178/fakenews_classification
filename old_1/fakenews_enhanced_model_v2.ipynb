{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gcsfs in /opt/conda/anaconda/lib/python3.6/site-packages (0.6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (2.23.0)\n",
      "Requirement already satisfied: google-auth>=1.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (1.11.3)\n",
      "Requirement already satisfied: decorator in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (4.4.2)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (0.4.1)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (0.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (3.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (1.14.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (4.0.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (4.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (46.0.0.post20200309)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/anaconda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.10\n",
      "CPU times: user 56.2 ms, sys: 8.48 ms, total: 64.7 ms\n",
      "Wall time: 62.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import os, sys, time, json, re, string, datetime\n",
    "\n",
    "from pyspark import SparkContext, SparkConf, StorageLevel, keyword_only\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.param.shared import HasInputCol, HasInputCols, HasOutputCol, HasOutputCols, Param\n",
    "from pyspark.ml.feature import OneHotEncoder, HashingTF, IDF, Tokenizer, RegexTokenizer, NGram, CountVectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover, VectorAssembler, PCA, OneHotEncoderEstimator,StringIndexer\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier, MultilayerPerceptronClassifier\n",
    "\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.68 µs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-b477-m.asia-southeast1-a.c.weicheng.internal:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 7 ms, total: 7 ms\n",
      "Wall time: 8.48 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"fakenews_baseline_model\") \\\n",
    "        .config(\"spark.master\", \"yarn\") \\\n",
    "        .config(\"spark.submit.deployMode\", \"cluster\") \\\n",
    "        .config(\"spark.driver.memory\", \"24g\") \\\n",
    "        .config(\"spark.executor.instances\", \"5\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.executor.memory\", \"24g\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 217806 entries, 0 to 108010\n",
      "Data columns (total 7 columns):\n",
      "domain             217806 non-null object\n",
      "type               217806 non-null object\n",
      "content            217806 non-null object\n",
      "title              217806 non-null object\n",
      "authors            217806 non-null object\n",
      "authors_missing    217806 non-null object\n",
      "label              217806 non-null object\n",
      "dtypes: object(7)\n",
      "memory usage: 13.3+ MB\n",
      "None\n",
      "CPU times: user 8.84 s, sys: 2.12 s, total: 11 s\n",
      "Wall time: 23 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>authors_missing</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>Boehner presses to make tax cuts permanent\\n\\n...</td>\n",
       "      <td>Boehner presses to make tax cuts permanent</td>\n",
       "      <td>United Liberty</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>An Armed Good Samaritan Who Doesn’t Want The S...</td>\n",
       "      <td>An Armed Good Samaritan Who Doesn’t Want The S...</td>\n",
       "      <td>The Real Revo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>(Before It's News)\\n\\nby Rob Morphy\\n\\nLegends...</td>\n",
       "      <td>Village Of The Dead: The Anjikuni Mystery</td>\n",
       "      <td>Rob Morphy, Mort Amsel</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>teaparty.org</td>\n",
       "      <td>fake</td>\n",
       "      <td>(Breitbart) – With his hysterical gotcha attac...</td>\n",
       "      <td>Trump Calls Out Race-Baiting ABC News Reporter</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>Researchers develop new depression diagnosis a...</td>\n",
       "      <td>Researchers develop new depression diagnosis a...</td>\n",
       "      <td>Bel Marra Health</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108006</th>\n",
       "      <td>sports.yahoo.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>View photos\\nMichigan guard Zak Irvin, right, ...</td>\n",
       "      <td>No. 25 Michigan beats Mount St. Mary's 64-47</td>\n",
       "      <td>The Associated Press</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108007</th>\n",
       "      <td>uk.finance.yahoo.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>President-elect Donald Trump continued his scr...</td>\n",
       "      <td>Trump quotes Hillary Clinton, rages against Wi...</td>\n",
       "      <td>Maxwell Tani</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108008</th>\n",
       "      <td>www.yahoo.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>Holiday shoppers eager to snag big discounts t...</td>\n",
       "      <td>Dialing up deals: Black Friday online sales hi...</td>\n",
       "      <td>ALEX VEIGA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108009</th>\n",
       "      <td>www.reuters.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>Kabul police raid shisha cafes in crackdown on...</td>\n",
       "      <td>Kabul police raid shisha cafes in crackdown on...</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108010</th>\n",
       "      <td>m.mlb.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>2016-17 MLB.com Hot Stove \\n© 2001- 2016 MLB A...</td>\n",
       "      <td>Possibility of Granderson in CF | MLB.com</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>217806 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      domain      type  \\\n",
       "0          beforeitsnews.com      fake   \n",
       "1          beforeitsnews.com      fake   \n",
       "2          beforeitsnews.com      fake   \n",
       "3               teaparty.org      fake   \n",
       "4          beforeitsnews.com      fake   \n",
       "...                      ...       ...   \n",
       "108006      sports.yahoo.com  reliable   \n",
       "108007  uk.finance.yahoo.com  reliable   \n",
       "108008         www.yahoo.com  reliable   \n",
       "108009       www.reuters.com  reliable   \n",
       "108010             m.mlb.com  reliable   \n",
       "\n",
       "                                                  content  \\\n",
       "0       Boehner presses to make tax cuts permanent\\n\\n...   \n",
       "1       An Armed Good Samaritan Who Doesn’t Want The S...   \n",
       "2       (Before It's News)\\n\\nby Rob Morphy\\n\\nLegends...   \n",
       "3       (Breitbart) – With his hysterical gotcha attac...   \n",
       "4       Researchers develop new depression diagnosis a...   \n",
       "...                                                   ...   \n",
       "108006  View photos\\nMichigan guard Zak Irvin, right, ...   \n",
       "108007  President-elect Donald Trump continued his scr...   \n",
       "108008  Holiday shoppers eager to snag big discounts t...   \n",
       "108009  Kabul police raid shisha cafes in crackdown on...   \n",
       "108010  2016-17 MLB.com Hot Stove \\n© 2001- 2016 MLB A...   \n",
       "\n",
       "                                                    title  \\\n",
       "0              Boehner presses to make tax cuts permanent   \n",
       "1       An Armed Good Samaritan Who Doesn’t Want The S...   \n",
       "2               Village Of The Dead: The Anjikuni Mystery   \n",
       "3          Trump Calls Out Race-Baiting ABC News Reporter   \n",
       "4       Researchers develop new depression diagnosis a...   \n",
       "...                                                   ...   \n",
       "108006       No. 25 Michigan beats Mount St. Mary's 64-47   \n",
       "108007  Trump quotes Hillary Clinton, rages against Wi...   \n",
       "108008  Dialing up deals: Black Friday online sales hi...   \n",
       "108009  Kabul police raid shisha cafes in crackdown on...   \n",
       "108010          Possibility of Granderson in CF | MLB.com   \n",
       "\n",
       "                       authors authors_missing label  \n",
       "0               United Liberty               0     1  \n",
       "1                The Real Revo               0     1  \n",
       "2       Rob Morphy, Mort Amsel               0     1  \n",
       "3                          nan               1     1  \n",
       "4             Bel Marra Health               0     1  \n",
       "...                        ...             ...   ...  \n",
       "108006    The Associated Press               0     0  \n",
       "108007            Maxwell Tani               0     0  \n",
       "108008              ALEX VEIGA               0     0  \n",
       "108009                     nan               1     0  \n",
       "108010                     nan               1     0  \n",
       "\n",
       "[217806 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_fake_train = pd.read_csv('gs://dataproc-6ca41800-27b4-47d5-abee-55c011dfa389-asia-southeast1/data/fake-news/100k_fake_news_cleaned_dataset.csv')\n",
    "df_fake_train[['authors_missing']] = df_fake_train[['authors_missing']].astype(int)\n",
    "df_fake_train['label'] = 1\n",
    "df_reliable_train = pd.read_csv('gs://dataproc-6ca41800-27b4-47d5-abee-55c011dfa389-asia-southeast1/data/fake-news/100k_reliable_news_cleaned_dataset.csv')\n",
    "df_reliable_train[['authors_missing']] = df_reliable_train[['authors_missing']].astype(int)\n",
    "df_reliable_train['label'] = 0\n",
    "df_news_train = pd.concat([df_fake_train, df_reliable_train])\n",
    "df_news_train[df_news_train.columns] = df_news_train[df_news_train.columns].astype(str)\n",
    "print(df_news_train.info())\n",
    "df_news_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+--------------------+--------------------+--------------------+---------------+-----+\n",
      "|           domain|type|             content|               title|             authors|authors_missing|label|\n",
      "+-----------------+----+--------------------+--------------------+--------------------+---------------+-----+\n",
      "|beforeitsnews.com|fake|Boehner presses t...|Boehner presses t...|      United Liberty|              0|    1|\n",
      "|beforeitsnews.com|fake|An Armed Good Sam...|An Armed Good Sam...|       The Real Revo|              0|    1|\n",
      "|beforeitsnews.com|fake|(Before It's News...|Village Of The De...|Rob Morphy, Mort ...|              0|    1|\n",
      "|     teaparty.org|fake|(Breitbart) – Wit...|Trump Calls Out R...|                 nan|              1|    1|\n",
      "|beforeitsnews.com|fake|Researchers devel...|Researchers devel...|    Bel Marra Health|              0|    1|\n",
      "|beforeitsnews.com|fake|Trading Watch Lis...|Trading Watch Lis...|Bulls On Wall Street|              0|    1|\n",
      "|beforeitsnews.com|fake|Looks like today ...|Looks like today ...|      Protein Wisdom|              0|    1|\n",
      "|beforeitsnews.com|fake|On the paradox of...|On the paradox of...|The Adam Smith In...|              0|    1|\n",
      "|beforeitsnews.com|fake|% of readers thin...|Minuteman- Most I...|         Nc Renegade|              0|    1|\n",
      "|beforeitsnews.com|fake|Juniper to Trim E...|Juniper to Trim E...|Zacks Investment ...|              0|    1|\n",
      "+-----------------+----+--------------------+--------------------+--------------------+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 13.4 s, sys: 2.32 s, total: 15.7 s\n",
      "Wall time: 22.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_news = spark.createDataFrame(df_news_train)\n",
    "df_news = df_news.withColumn(\"label\", df_news[\"label\"].cast(IntegerType()))\n",
    "df_news = df_news.withColumn(\"authors_missing\", df_news[\"authors_missing\"].cast(IntegerType()))\n",
    "df_news.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.17 ms, sys: 1.01 ms, total: 6.18 ms\n",
      "Wall time: 404 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#remove empty content which will cause problem when transform the text\n",
    "df_news = df_news.filter(df_news.content != \"\")\n",
    "df_news = df_news.filter(df_news.title != \"\")\n",
    "df_news = df_news.dropDuplicates(['label', 'content', 'title', 'authors_missing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep type and content\n",
    "df_news = df_news.select('label', 'content', 'title','authors_missing')\n",
    "\n",
    "# split the dataset\n",
    "df_train, df_test = df_news.randomSplit([0.8, 0.2], seed=666)\n",
    "param_tuning = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 94 µs, sys: 18 µs, total: 112 µs\n",
      "Wall time: 119 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# customized transformer class to manually extract some counting based text features\n",
    "class ReviewContentTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=\"content\", outputCol=\"content_features\"):\n",
    "        super(ReviewContentTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        def f(s):\n",
    "            uppercase_count = 0\n",
    "            char_count = 0\n",
    "            for c in s:                \n",
    "                if c in string.ascii_uppercase:\n",
    "                    uppercase_count += 1\n",
    "                    char_count += 1\n",
    "                elif c in string.ascii_lowercase:\n",
    "                    char_count += 1\n",
    "            \n",
    "            text_len = len(s)\n",
    "            return Vectors.dense(text_len, char_count, \n",
    "                                 uppercase_count, uppercase_count / (char_count + 1e-10))\n",
    "\n",
    "        return dataset.withColumn(self.getOutputCol(), \n",
    "                                  F.udf(f, VectorUDT())(dataset[self.getInputCol()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 100 µs, sys: 20 µs, total: 120 µs\n",
      "Wall time: 126 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# customized transformer class to manually extract some counting based word features\n",
    "class ReviewWordsTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=\"content\", outputCol=\"content_features\"):\n",
    "        super(ReviewWordsTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        def f(words):    \n",
    "            word_count = len(words)\n",
    "            unique_word_count = len(set(words))\n",
    "            upper_words = []\n",
    "            for w in words:\n",
    "                if w.isupper():\n",
    "                    upper_words.append(w)\n",
    "            upper_word_count = len(set(upper_words))\n",
    "            unique_upper_word_count = len(upper_words)\n",
    "            return Vectors.dense(word_count, unique_word_count, unique_word_count / (word_count + 1e-10),\n",
    "                                 upper_word_count, upper_word_count / (word_count + 1e-10), \n",
    "                                 unique_upper_word_count, unique_upper_word_count / (upper_word_count + 1e-10))\n",
    "\n",
    "        return dataset.withColumn(self.getOutputCol(), \n",
    "                                  F.udf(f, VectorUDT())(dataset[self.getInputCol()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 µs, sys: 2 µs, total: 14 µs\n",
      "Wall time: 18.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# show model prediction performance on the given dataset\n",
    "def eval_model_perf(fitted_model, dataset, label_col=\"label\", prediction_col=\"prediction\", probability_col=\"probability\"):\n",
    "    pred_dataset = fitted_model.transform(dataset)\n",
    "    eval_dataset = pred_dataset.select(label_col, prediction_col, probability_col)\n",
    "    # model performance evaluation\n",
    "    metricNames = [\"accuracy\", \"f1\"]\n",
    "    model_eval = MulticlassClassificationEvaluator(predictionCol=prediction_col, labelCol=label_col)\n",
    "    for m in metricNames:\n",
    "        val = model_eval.evaluate(eval_dataset, {model_eval.metricName: m})\n",
    "        print(m, \" = \", val)\n",
    "    roc_eval = BinaryClassificationEvaluator(rawPredictionCol=probability_col, labelCol=label_col, metricName=\"areaUnderROC\")\n",
    "    print(\"AUC =\", roc_eval.evaluate(eval_dataset))    \n",
    "    return pred_dataset\n",
    "\n",
    "# show CV param tunning result\n",
    "def show_cv_results(cv_model):\n",
    "    for result, param in sorted(zip(cv_model.avgMetrics, cv_model.getEstimatorParamMaps()), reverse=True, key=lambda x: x[0]):\n",
    "        print(result, \" | \", param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1 µs, total: 7 µs\n",
      "Wall time: 11.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def run_models(df_train, df_test):\n",
    "    print(\"**********LogisticRegression**********\")\n",
    "    t = time.time()\n",
    "    lr_model = LogisticRegression(featuresCol='features', \n",
    "                                  labelCol='label', \n",
    "                                  predictionCol='prediction', \n",
    "                                  probabilityCol='probability', \n",
    "                                  rawPredictionCol='rawPrediction',\n",
    "                                  family='binomial', \n",
    "                                  fitIntercept=True, \n",
    "                                  threshold=0.5, \n",
    "                                  standardization=False, \n",
    "                                  maxIter=200, \n",
    "                                  regParam=0.005, \n",
    "                                  elasticNetParam=0, \n",
    "                                  tol=1e-06, \n",
    "                                  aggregationDepth=2)\n",
    "\n",
    "    lr_model = lr_model.fit(df_train)\n",
    "    \n",
    "    eval_model_perf(lr_model, df_test)\n",
    "    \n",
    "    print(\"time taken for LogisticRegression: \", str(datetime.timedelta(seconds=time.time() - t)))\n",
    "    t = time.time()\n",
    "\n",
    "    print(\"**********DecisionTreeClassifier**********\")\n",
    "    dt_model = DecisionTreeClassifier(featuresCol='features', \n",
    "                                      labelCol='label', \n",
    "                                      predictionCol='prediction', \n",
    "                                      probabilityCol='probability', \n",
    "                                      rawPredictionCol='rawPrediction', \n",
    "                                      maxDepth=10, maxBins=32, \n",
    "                                      minInstancesPerNode=1, \n",
    "                                      minInfoGain=0.0, \n",
    "                                      maxMemoryInMB=2048, \n",
    "                                      cacheNodeIds=True, \n",
    "                                      checkpointInterval=10,\n",
    "                                      impurity='gini', \n",
    "                                      seed=666)\n",
    "\n",
    "    dt_model = dt_model.fit(df_train)\n",
    "    eval_model_perf(dt_model, df_test)\n",
    "    print(\"time taken for DecisionTreeClassifier: \", str(datetime.timedelta(seconds=time.time() - t)))\n",
    "    t = time.time()\n",
    "    \n",
    "    print(\"**********RandomForestClassifier**********\")\n",
    "    rf_model = RandomForestClassifier(featuresCol='features', \n",
    "                                      labelCol='label', \n",
    "                                      predictionCol='prediction', \n",
    "                                      probabilityCol='probability', \n",
    "                                      rawPredictionCol='rawPrediction',\n",
    "                                      maxDepth=10, \n",
    "                                      maxBins=32, \n",
    "                                      minInstancesPerNode=1, \n",
    "                                      minInfoGain=0.0, \n",
    "                                      maxMemoryInMB=2048, \n",
    "                                      cacheNodeIds=True, \n",
    "                                      checkpointInterval=10, \n",
    "                                      impurity='gini', \n",
    "                                      numTrees=200, \n",
    "                                      featureSubsetStrategy='auto', \n",
    "                                      seed=666, \n",
    "                                      subsamplingRate=0.8)\n",
    "\n",
    "    rf_model = rf_model.fit(df_train)\n",
    "    eval_model_perf(rf_model, df_test)\n",
    "    print(\"time taken for RandomForestClassifier: \", str(datetime.timedelta(seconds=time.time() - t)))\n",
    "    t = time.time()\n",
    "    \n",
    "    print(\"**********GBTClassifier**********\")\n",
    "    gbt_model = GBTClassifier(featuresCol='features', \n",
    "                             labelCol='label', \n",
    "                             maxIter=250)\n",
    "\n",
    "    gbt_model = gbt_model.fit(df_train)\n",
    "    eval_model_perf(gbt_model, df_test)\n",
    "    print(\"time taken for GBTClassifier: \", str(datetime.timedelta(seconds=time.time() - t)))\n",
    "    t = time.time()    \n",
    "    \n",
    "    print(\"**********MultilayerPerceptronClassifier**********\")\n",
    "    mp_model = MultilayerPerceptronClassifier(featuresCol='features', \n",
    "                                              labelCol='label', \n",
    "                                              predictionCol='prediction', \n",
    "                                              layers=[4, 5, 4, 3],  \n",
    "                                              maxIter=100, \n",
    "                                              blockSize=128, \n",
    "                                              seed=1234)\n",
    "\n",
    "    mp_model = mp_model.fit(df_train)\n",
    "    eval_model_perf(rf_model, df_test)\n",
    "    print(\"time taken for MultilayerPerceptronClassifier: \", str(datetime.timedelta(seconds=time.time() - t)))\n",
    "    t = time.time()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def build_data_preproc_model_with_pca(vocab_size=5000):\n",
    "    preproc_steps = [\n",
    "        RegexTokenizer(inputCol=\"content\", outputCol=\"all_words\", pattern=r\"\\W\"),\n",
    "        StopWordsRemover(inputCol=\"all_words\", outputCol=\"words\"),\n",
    "        CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=vocab_size),\n",
    "        IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\"),\n",
    "        PCA(inputCol=\"tfidf_features\", outputCol=\"pca_features\", k=100),\n",
    "        \n",
    "        ReviewContentTransformer(inputCol=\"content\", outputCol=\"content_features\"),\n",
    "        ReviewWordsTransformer(inputCol=\"words\", outputCol=\"word_features\"),\n",
    "        \n",
    "        RegexTokenizer(inputCol=\"title\", outputCol=\"all_title_words\", pattern=r\"\\W\"),\n",
    "        StopWordsRemover(inputCol=\"all_title_words\", outputCol=\"title_words\"),\n",
    "        CountVectorizer(inputCol=\"title_words\", outputCol=\"title_tf_features\", vocabSize=100),\n",
    "        IDF(inputCol=\"title_tf_features\", outputCol=\"title_tfidf_features\"),\n",
    "        PCA(inputCol=\"title_tfidf_features\", outputCol=\"title_pca_features\", k=100),   \n",
    "        \n",
    "        StringIndexer(inputCol=\"authors_missing\", outputCol=\"authors_missing_indexed\", handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol=\"authors_missing_indexed\", outputCol=\"authors_missing_feature\"),\n",
    "        \n",
    "        VectorAssembler(inputCols=[\"pca_features\", \"title_pca_features\", \"title_tfidf_features\", \n",
    "                                   \"content_features\", \"word_features\", \"authors_missing_feature\"], \n",
    "                        outputCol=\"features\")\n",
    "    ]\n",
    "    return Pipeline(stages=preproc_steps)\n",
    "\n",
    "def build_data_preproc_model_without_pca(vocab_size=5000):\n",
    "    preproc_steps = [\n",
    "        RegexTokenizer(inputCol=\"content\", outputCol=\"all_words\", pattern=r\"\\W\"),\n",
    "        StopWordsRemover(inputCol=\"all_words\", outputCol=\"words\"),\n",
    "        \n",
    "        StringIndexer(inputCol=\"authors_missing\", outputCol=\"authors_missing_indexed\", handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol=\"authors_missing_indexed\", outputCol=\"authors_missing_feature\"),\n",
    "        \n",
    "        ReviewContentTransformer(inputCol=\"content\", outputCol=\"content_features\"),\n",
    "        ReviewWordsTransformer(inputCol=\"words\", outputCol=\"word_features\"),\n",
    "        \n",
    "        VectorAssembler(inputCols=[\"content_features\", \"word_features\", \"authors_missing_feature\"], outputCol=\"features\")\n",
    "    ]\n",
    "    return Pipeline(stages=preproc_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Run Models with PCA Features**********\n",
      "[Row(label=0, features=DenseVector([-18.8241, -3.4875, 1.7624, 4.4359, 3.5469, -5.2796, 1.0957, 1.7793, -5.1523, 2.0355, 0.7305, -4.1008, 1.4991, -2.0755, 1.9944, -4.8548, 1.0588, 5.061, -3.2008, -4.7146, 12.072, 4.5596, 0.6113, 2.2505, 1.8553, 3.8479, -6.1946, 2.9211, -8.9351, 1.606, -2.5601, -2.2386, -15.2543, 8.6931, 3.5452, -4.6838, 3.0176, 9.5615, -1.9669, -2.1753, -4.7353, 0.2118, -5.7341, -5.3679, -6.0063, 3.9125, 1.801, 9.1861, 2.8239, 1.2538, -1.4186, 7.1026, -1.094, -6.084, -1.8551, 5.504, 1.0316, -3.4852, -2.3373, -0.5905, -2.482, 1.5996, -8.4547, -1.1193, -5.0701, 7.5682, 4.9765, -1.8888, -0.4902, -5.4125, -0.1729, 1.7335, 4.8978, 2.9605, 3.0806, -2.9867, -4.0572, -6.3073, 3.4909, 6.2352, -1.0364, -1.0758, 3.2993, -0.6111, -2.1284, 3.8133, 4.0526, 5.2001, -5.7574, 0.2304, -0.2902, -0.5034, 6.2816, 3.214, 2.1842, -0.1246, -1.4028, -7.7809, -6.0749, -5.2691, -0.0254, 0.015, 0.0258, -0.0004, -0.0263, -0.0064, 0.028, -0.0451, 0.0339, 0.0256, -0.0202, 0.0109, 0.0817, -0.0944, 0.0296, 0.0229, -0.0625, -0.008, 0.0931, 0.054, 0.0125, -0.0759, -0.0746, -0.0443, -0.0287, -0.0836, -0.0827, -0.007, 0.0638, -0.0704, 0.0017, -0.214, 0.0421, 0.1484, 0.108, 0.0345, -0.1128, -0.0449, 0.1192, 0.127, -0.0193, 0.1545, 0.1614, 0.2107, -0.1062, -0.1038, 0.0758, 0.4466, 0.0156, -0.1538, 0.2159, -0.0409, 0.2015, 0.0302, -0.0985, 0.3082, 0.1974, 0.465, -0.5925, -1.0407, -0.6277, 3.3155, 0.8962, -0.5952, 0.9421, -0.2218, 0.8431, -0.3242, 0.278, -0.1427, 0.3832, 0.1409, 0.1705, -0.4164, -0.2991, -0.7337, 0.1477, -0.1187, 0.8316, -0.5818, 0.6081, -1.543, -0.5553, 0.3217, -0.5736, -1.2864, 0.7713, -0.2574, 0.088, 0.0652, -0.067, 0.2432, 0.0012, -0.0274, 0.0068, 0.0369, 0.0195, 0.0023, 0.0026, -0.0003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.9776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5741.0, 4510.0, 182.0, 0.0404, 549.0, 329.0, 0.5993, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]))]\n",
      "[Row(label=0, features=SparseVector(313, {0: -5.742, 1: -0.2963, 2: -1.5631, 3: 0.9896, 4: 0.0161, 5: -1.5343, 6: -0.8203, 7: -0.2657, 8: -0.9079, 9: 1.4554, 10: 0.7874, 11: -1.767, 12: 1.0369, 13: 0.4861, 14: -0.4247, 15: -2.975, 16: 0.1313, 17: -0.0692, 18: -0.3773, 19: 0.0205, 20: 0.71, 21: 2.1786, 22: 0.7482, 23: 2.7273, 24: 0.2166, 25: 0.8981, 26: 0.8976, 27: 0.0259, 28: -1.2533, 29: -3.136, 30: 1.0791, 31: 4.0402, 32: 0.5682, 33: 3.4143, 34: -1.9823, 35: -0.3201, 36: 1.4143, 37: 2.527, 38: 0.3666, 39: 3.0786, 40: 4.4292, 41: 2.9771, 42: -3.5576, 43: -0.5086, 44: -4.6681, 45: -1.5777, 46: -1.3037, 47: -2.7932, 48: 4.8385, 49: -3.6598, 50: -1.3157, 51: -0.3614, 52: 1.7376, 53: -1.412, 54: -2.0861, 55: -0.9596, 56: -0.0259, 57: 1.8485, 58: 0.9744, 59: 0.5608, 60: -0.966, 61: -0.6144, 62: -1.5953, 63: 0.6512, 64: 3.511, 65: -2.2601, 66: 0.2768, 67: -0.6296, 68: 0.2252, 69: 3.6169, 70: -3.8172, 71: 0.9922, 72: 1.3853, 73: -1.943, 74: -1.5149, 75: -0.9768, 76: 2.113, 77: -1.2403, 78: -1.9524, 79: 0.2643, 80: -1.71, 81: 3.3675, 82: 3.098, 83: -0.3809, 84: -1.1055, 85: 1.3985, 86: 3.8737, 87: 1.0742, 88: -2.6099, 89: 2.8602, 90: 2.5993, 91: 1.2784, 92: 1.5556, 93: -0.5201, 94: -4.3555, 95: -3.1908, 96: 1.0721, 97: 3.4574, 98: 3.023, 99: 0.6747, 300: 1862.0, 301: 1481.0, 302: 104.0, 303: 0.0702, 304: 182.0, 305: 146.0, 306: 0.8022, 312: 1.0}))]\n",
      "**********LogisticRegression**********\n",
      "accuracy  =  0.8900997414111562\n",
      "f1  =  0.8899885592389403\n",
      "AUC = 0.952914960989118\n",
      "time taken for LogisticRegression:  0:04:23.345775\n",
      "**********DecisionTreeClassifier**********\n",
      "accuracy  =  0.8287079793128925\n",
      "f1  =  0.8286818269563927\n",
      "AUC = 0.8988270725746094\n",
      "time taken for DecisionTreeClassifier:  0:04:21.389089\n",
      "**********RandomForestClassifier**********\n",
      "accuracy  =  0.865487624676764\n",
      "f1  =  0.8654546119989036\n",
      "AUC = 0.9414119749159164\n",
      "time taken for RandomForestClassifier:  0:10:13.294548\n",
      "**********GBTClassifier**********\n",
      "accuracy  =  0.9042066863686739\n",
      "f1  =  0.9042048600001236\n",
      "AUC = 0.9683020455059095\n",
      "time taken for GBTClassifier:  1:14:16.733835\n",
      "**********MultilayerPerceptronClassifier**********\n",
      "accuracy  =  0.865487624676764\n",
      "f1  =  0.8654546119989036\n",
      "AUC = 0.9414119749159171\n",
      "time taken for MultilayerPerceptronClassifier:  0:04:17.148360\n",
      "CPU times: user 45.9 s, sys: 21.5 s, total: 1min 7s\n",
      "Wall time: 1h 40min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"**********Run Models with PCA Features**********\")\n",
    "# generate the features to be used for model training\n",
    "preproc_model = build_data_preproc_model_with_pca(2000).fit(df_train)\n",
    "df_train_pca = preproc_model.transform(df_train).select(\"label\", \"features\")\n",
    "print(df_train_pca.take(1))\n",
    "df_test_pca = preproc_model.transform(df_test).select(\"label\", \"features\")\n",
    "print(df_test_pca.take(1))\n",
    "run_models(df_train_pca, df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Run Models without PCA Features**********\n",
      "[Row(label=0, features=DenseVector([5741.0, 4510.0, 182.0, 0.0404, 549.0, 329.0, 0.5993, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]))]\n",
      "[Row(label=0, features=DenseVector([1862.0, 1481.0, 104.0, 0.0702, 182.0, 146.0, 0.8022, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]))]\n",
      "accuracy  =  0.5702807536017732\n",
      "f1  =  0.5659001574227993\n",
      "AUC = 0.6287591998742492\n",
      "CPU times: user 389 ms, sys: 184 ms, total: 573 ms\n",
      "Wall time: 2min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "print(\"**********Run Models without PCA Features**********\")\n",
    "# generate the features to be used for model training\n",
    "preproc_model = build_data_preproc_model_without_pca(2000).fit(df_train)\n",
    "df_train_wo_pca = preproc_model.transform(df_train).select(\"label\", \"features\")\n",
    "print(df_train_wo_pca.take(1))\n",
    "df_test_wo_pca = preproc_model.transform(df_test).select(\"label\", \"features\")\n",
    "print(df_test_wo_pca.take(1))\n",
    "\n",
    "nb_model = NaiveBayes(featuresCol='features', \n",
    "                      labelCol='label', \n",
    "                      predictionCol='prediction', \n",
    "                      probabilityCol='probability', \n",
    "                      rawPredictionCol='rawPrediction', \n",
    "                      smoothing=1, \n",
    "                      modelType='multinomial')\n",
    "\n",
    "nb_model = nb_model.fit(df_train_wo_pca)\n",
    "eval_model_perf(nb_model, df_test_wo_pca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

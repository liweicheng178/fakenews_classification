{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.10\n",
      "CPU times: user 51.1 ms, sys: 18.8 ms, total: 69.9 ms\n",
      "Wall time: 67.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import os, sys, time, json, re, string, datetime\n",
    "\n",
    "from pyspark import SparkContext, SparkConf, StorageLevel, keyword_only\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.param.shared import HasInputCol, HasInputCols, HasOutputCol, HasOutputCols, Param\n",
    "from pyspark.ml.feature import OneHotEncoder, HashingTF, IDF, Tokenizer, RegexTokenizer, NGram, CountVectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover, VectorAssembler, PCA, OneHotEncoderEstimator,StringIndexer\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier, MultilayerPerceptronClassifier\n",
    "\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-8f64-m.asia-southeast1-a.c.weicheng.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.68 ms, sys: 97 µs, total: 8.78 ms\n",
      "Wall time: 11.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"fakenews_enhanced_model\") \\\n",
    "        .config(\"spark.master\", \"yarn\") \\\n",
    "        .config(\"spark.submit.deployMode\", \"cluster\") \\\n",
    "        .config(\"spark.driver.memory\", \"15g\") \\\n",
    "        .config(\"spark.executor.instances\", \"5\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.3 ms, sys: 777 µs, total: 14 ms\n",
      "Wall time: 6.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fakenews_path=\"gs://dataproc-6ca41800-27b4-47d5-abee-55c011dfa389-asia-southeast1/data/fake-news/\"\n",
    "fakenews_data_path = fakenews_path + \"two_million_rows_news_cleaned_2018_02_13_pyspark.csv\"\n",
    "df_news = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .option(\"delimiter\", '#') \\\n",
    "                    .load(fakenews_data_path)\n",
    "\n",
    "#remove empty content which will cause problem when transform the text\n",
    "df_news = df_news.filter(df_news.type != \"\")\n",
    "df_news = df_news.filter(df_news.content != \"\")\n",
    "df_news = df_news.filter(df_news.domain != \"\")\n",
    "df_news = df_news.filter(df_news.title != \"\")\n",
    "df_news = df_news.filter(df_news.authors != \"\")\n",
    "df_news = df_news.dropDuplicates(['type', 'content', 'title', 'authors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep type and content\n",
    "df_news = df_news.select('type', 'content', 'domain', 'title','authors')\n",
    "split_authors_col = F.split(df_news['authors'], ', ')\n",
    "df_news = df_news.withColumn('author1', split_authors_col.getItem(0))\n",
    "df_news = df_news.withColumn('author2', split_authors_col.getItem(1))\n",
    "\n",
    "# add binary label\n",
    "df_news = df_news.withColumn(\"label\", F.when(F.col(\"type\") == 'fake', 1).otherwise(0))\n",
    "\n",
    "#facing out of memory issue in google cluster for 5 instance * 15 GB, so sampling with 10K fake news and non-fake news\n",
    "row_count = 10000\n",
    "df_news_fake = df_news.filter(df_news.type == 'fake').limit(row_count)\n",
    "df_news_nonfake = df_news.filter(df_news.type != 'fake').limit(row_count)\n",
    "df_news = df_news_fake.union(df_news_nonfake)\n",
    "\n",
    "# split the dataset\n",
    "df_train, df_test = df_news.randomSplit([0.8, 0.2], seed=666)\n",
    "param_tuning = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 95 µs, sys: 13 µs, total: 108 µs\n",
      "Wall time: 114 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# customized transformer class to manually extract some counting based text features\n",
    "class ReviewContentTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=\"content\", outputCol=\"content_features\"):\n",
    "        super(ReviewContentTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        def f(s):\n",
    "            uppercase_count = 0\n",
    "            char_count = 0\n",
    "            for c in s:                \n",
    "                if c in string.ascii_uppercase:\n",
    "                    uppercase_count += 1\n",
    "                    char_count += 1\n",
    "                elif c in string.ascii_lowercase:\n",
    "                    char_count += 1\n",
    "            \n",
    "            text_len = len(s)\n",
    "            return Vectors.dense(text_len, char_count, \n",
    "                                 uppercase_count, uppercase_count / (char_count + 1e-10))\n",
    "\n",
    "        return dataset.withColumn(self.getOutputCol(), \n",
    "                                  F.udf(f, VectorUDT())(dataset[self.getInputCol()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 71 µs, sys: 10 µs, total: 81 µs\n",
      "Wall time: 85.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# customized transformer class to manually extract some counting based word features\n",
    "class ReviewWordsTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=\"content\", outputCol=\"content_features\"):\n",
    "        super(ReviewWordsTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        def f(words):    \n",
    "            word_count = len(words)\n",
    "            unique_word_count = len(set(words))\n",
    "            upper_words = []\n",
    "            for w in words:\n",
    "                if w.isupper():\n",
    "                    upper_words.append(w)\n",
    "            upper_word_count = len(set(upper_words))\n",
    "            unique_upper_word_count = len(upper_words)\n",
    "            return Vectors.dense(word_count, unique_word_count, unique_word_count / (word_count + 1e-10),\n",
    "                                 upper_word_count, upper_word_count / (word_count + 1e-10), \n",
    "                                 unique_upper_word_count, unique_upper_word_count / (upper_word_count + 1e-10))\n",
    "\n",
    "        return dataset.withColumn(self.getOutputCol(), \n",
    "                                  F.udf(f, VectorUDT())(dataset[self.getInputCol()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1 µs, total: 7 µs\n",
      "Wall time: 11.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# show model prediction performance on the given dataset\n",
    "def eval_model_perf(fitted_model, dataset, label_col=\"label\", prediction_col=\"prediction\", probability_col=\"probability\"):\n",
    "    pred_dataset = fitted_model.transform(dataset)\n",
    "    eval_dataset = pred_dataset.select(label_col, prediction_col, probability_col)\n",
    "    # model performance evaluation\n",
    "    metricNames = [\"accuracy\", \"f1\"]\n",
    "    model_eval = MulticlassClassificationEvaluator(predictionCol=prediction_col, labelCol=label_col)\n",
    "    for m in metricNames:\n",
    "        val = model_eval.evaluate(eval_dataset, {model_eval.metricName: m})\n",
    "        print(m, \" = \", val)\n",
    "    roc_eval = BinaryClassificationEvaluator(rawPredictionCol=probability_col, labelCol=label_col, metricName=\"areaUnderROC\")\n",
    "    print(\"AUC =\", roc_eval.evaluate(eval_dataset))    \n",
    "    return pred_dataset\n",
    "\n",
    "# show CV param tunning result\n",
    "def show_cv_results(cv_model):\n",
    "    for result, param in sorted(zip(cv_model.avgMetrics, cv_model.getEstimatorParamMaps()), reverse=True, key=lambda x: x[0]):\n",
    "        print(result, \" | \", param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 1 µs, total: 8 µs\n",
      "Wall time: 12.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def run_models(df_train, df_test):\n",
    "    print(\"**********LogisticRegression**********\")\n",
    "    t = time.time()\n",
    "    lr_model = LogisticRegression(featuresCol='features', \n",
    "                                  labelCol='label', \n",
    "                                  predictionCol='prediction', \n",
    "                                  probabilityCol='probability', \n",
    "                                  rawPredictionCol='rawPrediction',\n",
    "                                  family='binomial', \n",
    "                                  fitIntercept=True, \n",
    "                                  threshold=0.5, \n",
    "                                  standardization=False, \n",
    "                                  maxIter=200, \n",
    "                                  regParam=0.005, \n",
    "                                  elasticNetParam=0, \n",
    "                                  tol=1e-06, \n",
    "                                  aggregationDepth=2)\n",
    "\n",
    "    lr_model = lr_model.fit(df_train)\n",
    "    \n",
    "    eval_model_perf(lr_model, df_test)\n",
    "    \n",
    "    print(\"time taken for LogisticRegression: \", str(datetime.timedelta(seconds=time.time() - t)))\n",
    "    t = time.time()\n",
    "\n",
    "    # BELOW CODE IS USED ONLY FOR PARAM TUNNING\n",
    "    # # grid search params tunning\n",
    "    # paramGrid = ParamGridBuilder() \\\n",
    "    #     .addGrid(lr_model.regParam, [0.001, 0.005]) \\\n",
    "    #     .addGrid(lr_model.standardization, [False, True]) \\\n",
    "    #     .build()\n",
    "    # # cross validator model\n",
    "    # crossval = CrossValidator(estimator=lr_model, \n",
    "    # #                           evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"),\n",
    "    #                           evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"label\", metricName=\"areaUnderROC\"),\n",
    "    #                           estimatorParamMaps=paramGrid,\n",
    "    #                           numFolds=2, parallelism = 2,\n",
    "    #                           seed=666)\n",
    "    # # run it!\n",
    "    # crossval_model = crossval.fit(df_train)\n",
    "    # # evaluate performance\n",
    "    # show_cv_results(crossval_model)\n",
    "    # eval_model_perf(crossval_model, df_test)\n",
    "    # selected_model = crossval_model.bestModel\n",
    "    # print(selected_model.explainParams())\n",
    "    print(\"**********DecisionTreeClassifier**********\")\n",
    "    dt_model = DecisionTreeClassifier(featuresCol='features', \n",
    "                                      labelCol='label', \n",
    "                                      predictionCol='prediction', \n",
    "                                      probabilityCol='probability', \n",
    "                                      rawPredictionCol='rawPrediction', \n",
    "                                      maxDepth=10, maxBins=32, \n",
    "                                      minInstancesPerNode=1, \n",
    "                                      minInfoGain=0.0, \n",
    "                                      maxMemoryInMB=2048, \n",
    "                                      cacheNodeIds=True, \n",
    "                                      checkpointInterval=10,\n",
    "                                      impurity='gini', \n",
    "                                      seed=666)\n",
    "\n",
    "    dt_model = dt_model.fit(df_train)\n",
    "    eval_model_perf(dt_model, df_test)\n",
    "    print(\"time taken for DecisionTreeClassifier: \", str(datetime.timedelta(seconds=time.time() - t)))\n",
    "    t = time.time()\n",
    "    \n",
    "    print(\"**********RandomForestClassifier**********\")\n",
    "    rf_model = RandomForestClassifier(featuresCol='features', \n",
    "                                      labelCol='label', \n",
    "                                      predictionCol='prediction', \n",
    "                                      probabilityCol='probability', \n",
    "                                      rawPredictionCol='rawPrediction',\n",
    "                                      maxDepth=10, \n",
    "                                      maxBins=32, \n",
    "                                      minInstancesPerNode=1, \n",
    "                                      minInfoGain=0.0, \n",
    "                                      maxMemoryInMB=2048, \n",
    "                                      cacheNodeIds=True, \n",
    "                                      checkpointInterval=10, \n",
    "                                      impurity='gini', \n",
    "                                      numTrees=200, \n",
    "                                      featureSubsetStrategy='auto', \n",
    "                                      seed=666, \n",
    "                                      subsamplingRate=0.8)\n",
    "\n",
    "    rf_model = rf_model.fit(df_train)\n",
    "    eval_model_perf(rf_model, df_test)\n",
    "    print(\"time taken for RandomForestClassifier: \", str(datetime.timedelta(seconds=time.time() - t)))\n",
    "    t = time.time()\n",
    "    \n",
    "    print(\"**********GBTClassifier**********\")\n",
    "    gbt_model = GBTClassifier(featuresCol='features', \n",
    "                             labelCol='label', \n",
    "                             maxIter=250)\n",
    "\n",
    "    gbt_model = gbt_model.fit(df_train)\n",
    "    eval_model_perf(gbt_model, df_test)\n",
    "    print(\"time taken for GBTClassifier: \", str(datetime.timedelta(seconds=time.time() - t)))\n",
    "    t = time.time()    \n",
    "    \n",
    "    print(\"**********MultilayerPerceptronClassifier**********\")\n",
    "    mp_model = MultilayerPerceptronClassifier(featuresCol='features', \n",
    "                                              labelCol='label', \n",
    "                                              predictionCol='prediction', \n",
    "                                              layers=[4, 5, 4, 3],  \n",
    "                                              maxIter=100, \n",
    "                                              blockSize=128, \n",
    "                                              seed=1234)\n",
    "\n",
    "    mp_model = mp_model.fit(df_train)\n",
    "    eval_model_perf(rf_model, df_test)\n",
    "    print(\"time taken for MultilayerPerceptronClassifier: \", str(datetime.timedelta(seconds=time.time() - t)))\n",
    "    t = time.time()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def build_data_preproc_model_with_pca(vocab_size=5000):\n",
    "    preproc_steps = [\n",
    "        RegexTokenizer(inputCol=\"content\", outputCol=\"all_words\", pattern=r\"\\W\"),\n",
    "        StopWordsRemover(inputCol=\"all_words\", outputCol=\"words\"),\n",
    "        CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=vocab_size),\n",
    "        IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\"),\n",
    "        PCA(inputCol=\"tfidf_features\", outputCol=\"pca_features\", k=100),\n",
    "        \n",
    "        ReviewContentTransformer(inputCol=\"content\", outputCol=\"content_features\"),\n",
    "        ReviewWordsTransformer(inputCol=\"words\", outputCol=\"word_features\"),\n",
    "        \n",
    "        RegexTokenizer(inputCol=\"title\", outputCol=\"all_title_words\", pattern=r\"\\W\"),\n",
    "        StopWordsRemover(inputCol=\"all_title_words\", outputCol=\"title_words\"),\n",
    "        CountVectorizer(inputCol=\"title_words\", outputCol=\"title_tf_features\", vocabSize=100),\n",
    "        IDF(inputCol=\"title_tf_features\", outputCol=\"title_tfidf_features\"),\n",
    "        PCA(inputCol=\"title_tfidf_features\", outputCol=\"title_pca_features\", k=100),        \n",
    "\n",
    "        #StringIndexer(inputCol=\"domain\", outputCol=\"domain_indexed\", handleInvalid='keep'),\n",
    "        #OneHotEncoder(inputCol=\"domain_indexed\", outputCol=\"domain_feature\"),\n",
    "        \n",
    "        StringIndexer(inputCol=\"author1\", outputCol=\"author1_indexed\", handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol=\"author1_indexed\", outputCol=\"author1_feature\"),\n",
    "        \n",
    "        StringIndexer(inputCol=\"author2\", outputCol=\"author2_indexed\", handleInvalid='keep'),        \n",
    "        OneHotEncoder(inputCol=\"author2_indexed\", outputCol=\"author2_feature\"),\n",
    "        \n",
    "        VectorAssembler(inputCols=[\"pca_features\", \"title_pca_features\", \n",
    "                                   \"title_tfidf_features\", \n",
    "                                   \"content_features\", \"word_features\",\n",
    "                                   \"author1_feature\", \"author2_feature\"], \n",
    "                        outputCol=\"features\")\n",
    "    ]\n",
    "    return Pipeline(stages=preproc_steps)\n",
    "\n",
    "def build_data_preproc_model_without_pca(vocab_size=5000):\n",
    "    preproc_steps = [\n",
    "        RegexTokenizer(inputCol=\"content\", outputCol=\"all_words\", pattern=r\"\\W\"),\n",
    "        StopWordsRemover(inputCol=\"all_words\", outputCol=\"words\"),\n",
    "        \n",
    "        #StringIndexer(inputCol=\"domain\", outputCol=\"domain_indexed\", handleInvalid='keep'),\n",
    "        #OneHotEncoder(inputCol=\"domain_indexed\", outputCol=\"domain_feature\"),\n",
    "        \n",
    "        StringIndexer(inputCol=\"author1\", outputCol=\"author1_indexed\", handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol=\"author1_indexed\", outputCol=\"author1_feature\"),\n",
    "        \n",
    "        StringIndexer(inputCol=\"author2\", outputCol=\"author2_indexed\", handleInvalid='keep'),        \n",
    "        OneHotEncoder(inputCol=\"author2_indexed\", outputCol=\"author2_feature\"),\n",
    "        \n",
    "        ReviewContentTransformer(inputCol=\"content\", outputCol=\"content_features\"),\n",
    "        ReviewWordsTransformer(inputCol=\"words\", outputCol=\"word_features\"),\n",
    "        \n",
    "        VectorAssembler(inputCols=[\"content_features\", \"word_features\", \"author1_feature\", \"author2_feature\"], \n",
    "                        outputCol=\"features\")\n",
    "    ]\n",
    "    return Pipeline(stages=preproc_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Run Models with PCA Features**********\n",
      "[Row(label=1, features=SparseVector(4756, {0: -1.7782, 1: 9.3616, 2: 2.8335, 3: 0.023, 4: -0.0543, 5: -4.8647, 6: 0.692, 7: -0.3924, 8: 2.2732, 9: -2.7471, 10: -0.408, 11: -1.9089, 12: -0.2284, 13: 0.0612, 14: 2.0305, 15: 0.3986, 16: 1.3213, 17: 0.6804, 18: 1.9046, 19: -1.4687, 20: -1.7719, 21: 1.4647, 22: 0.8811, 23: -4.2677, 24: 2.3876, 25: 0.551, 26: 0.9296, 27: 0.1785, 28: 1.4777, 29: 0.7244, 30: -3.1253, 31: -1.1222, 32: 0.2349, 33: 1.643, 34: 0.2642, 35: -0.0612, 36: -1.5604, 37: -0.3543, 38: -0.9313, 39: -1.17, 40: -2.2377, 41: -0.2121, 42: -0.0275, 43: -0.3359, 44: 1.8031, 45: 2.026, 46: 1.1528, 47: 0.4522, 48: 1.8751, 49: 0.6921, 50: 1.6989, 51: 3.7284, 52: 0.1599, 53: -0.7542, 54: -3.0221, 55: 0.4517, 56: 2.9104, 57: 1.1204, 58: -0.0651, 59: -0.3746, 60: -0.1126, 61: -0.845, 62: -0.8866, 63: 1.4559, 64: -0.0799, 65: 0.9001, 66: -0.2304, 67: 1.331, 68: -1.0528, 69: 0.513, 70: 1.7064, 71: 1.8573, 72: -1.2749, 73: 0.5488, 74: -1.0466, 75: -1.3477, 76: -0.2766, 77: 0.6284, 78: 0.0174, 79: 0.1406, 80: -2.3548, 81: -1.3733, 82: 0.6629, 83: -1.2108, 84: 0.512, 85: -0.4168, 86: 0.6384, 87: -0.7554, 88: 1.6202, 89: 0.2733, 90: 0.2985, 91: -1.7261, 92: -0.4624, 93: -0.0667, 94: 1.6381, 95: -0.5288, 96: 0.4677, 97: 1.4757, 98: 1.043, 99: -0.5755, 100: 0.0572, 101: 0.0201, 102: -0.027, 103: 0.0088, 104: 0.0045, 105: 0.0042, 106: -0.0692, 107: 0.0513, 108: -0.1223, 109: 0.0439, 110: -0.0268, 111: 0.1433, 112: -0.2624, 113: -0.2469, 114: -0.1032, 115: -0.5395, 116: -0.279, 117: -0.2091, 118: -0.2969, 119: 0.3833, 120: 0.3787, 121: 0.5675, 122: -0.6173, 123: -0.1226, 124: 0.3713, 125: -1.6267, 126: -0.6239, 127: 0.5452, 128: 2.132, 129: 1.4256, 130: -0.9958, 131: 1.2367, 132: -1.4263, 133: 0.7808, 134: -0.0562, 135: 0.6473, 136: -0.1166, 137: -1.5008, 138: -0.8153, 139: 0.3308, 140: -0.1323, 141: -0.2594, 142: -0.0033, 143: 0.4545, 144: -0.5862, 145: 0.2857, 146: 0.4164, 147: -0.2096, 148: 0.2161, 149: 0.0174, 150: -0.0691, 151: -0.1632, 152: -0.3458, 153: -0.0334, 154: -0.233, 155: 0.1255, 156: 0.1127, 157: -0.1774, 158: 0.033, 159: 0.0263, 160: 0.0684, 161: -0.092, 162: 0.0019, 163: -0.106, 164: -0.1916, 165: 0.2761, 166: 0.0511, 167: 0.1985, 168: -0.2876, 169: -0.2172, 170: 0.3641, 171: -0.1556, 172: -0.1593, 173: 0.1657, 174: -0.0916, 175: -0.0566, 176: 0.0088, 177: 0.0509, 178: -0.3562, 179: -0.0881, 180: -0.1659, 181: -0.0943, 182: -0.0651, 183: 0.1501, 184: -0.3457, 185: -0.1872, 186: 0.059, 187: -0.0828, 188: 0.1664, 189: 0.0248, 190: 0.1164, 191: 0.0673, 192: 0.004, 193: 0.0261, 194: -0.0223, 195: -0.0156, 196: -0.0119, 197: -0.0137, 198: -0.075, 199: -0.0084, 244: 4.7512, 300: 3393.0, 301: 2653.0, 302: 97.0, 303: 0.0366, 304: 311.0, 305: 218.0, 306: 0.701, 4077: 1.0}))]\n",
      "[Row(label=1, features=SparseVector(4756, {0: -3.6155, 1: 18.1849, 2: 7.5892, 3: 1.0844, 4: -0.5928, 5: -4.9899, 6: 2.3184, 7: -2.9438, 8: 1.7467, 9: -3.7555, 10: 0.1566, 11: -0.2164, 12: 1.9195, 13: 0.6031, 14: -1.6336, 15: 2.4702, 16: 2.789, 17: -2.1587, 18: 4.8401, 19: 1.5524, 20: -2.4687, 21: 1.6339, 22: -2.5879, 23: -1.6085, 24: 3.0685, 25: 4.0941, 26: 3.5794, 27: -0.5433, 28: -3.3715, 29: 0.8492, 30: -0.3493, 31: 8.4984, 32: 2.0009, 33: 3.1609, 34: -3.5969, 35: 1.3107, 36: -4.2986, 37: 0.2941, 38: -3.5093, 39: -4.2549, 40: -1.5805, 41: -2.237, 42: -0.814, 43: -0.0285, 44: -0.8356, 45: 0.0874, 46: 4.7162, 47: -3.6817, 48: -1.0333, 49: -1.7625, 50: 3.9712, 51: 1.0718, 52: -3.3775, 53: -0.3874, 54: -4.2809, 55: -1.1767, 56: 6.3113, 57: 0.0408, 58: -0.0855, 59: -4.3064, 60: 2.1718, 61: 1.1086, 62: -1.459, 63: -1.962, 64: -0.9106, 65: 4.6559, 66: 1.6268, 67: 0.3931, 68: -2.8698, 69: -1.3187, 70: 2.3029, 71: -1.1794, 72: -4.1587, 73: 1.2317, 74: -1.4462, 75: -2.3916, 76: -0.7922, 77: 2.3563, 78: -0.9434, 79: 3.1393, 80: -4.0384, 81: -4.6207, 82: 0.4762, 83: -0.5853, 84: 1.5861, 85: -0.1201, 86: 2.0692, 87: 0.9207, 88: 2.5483, 89: -0.212, 90: 2.5254, 91: -0.7439, 92: 4.1108, 93: 2.9076, 94: 0.4111, 95: -1.2454, 96: 4.301, 97: 3.3375, 98: 2.2109, 99: -4.8513, 100: 0.0366, 101: 0.0153, 102: 0.0935, 103: -0.1763, 104: 0.0001, 105: 0.0439, 106: -0.0548, 107: -0.0529, 108: -0.0743, 109: -0.0426, 110: 0.0088, 111: 0.0329, 112: 0.0369, 113: -0.002, 114: -0.0156, 115: 0.0676, 116: 0.1321, 117: -0.1151, 118: -0.1912, 119: -0.0434, 120: -0.1694, 121: -0.0466, 122: 0.099, 123: 0.0099, 124: 0.1179, 125: 0.127, 126: 0.1564, 127: 0.4038, 128: 0.4076, 129: 0.254, 130: -0.138, 131: -0.0866, 132: 0.3764, 133: -0.0489, 134: -0.0465, 135: -0.1239, 136: 0.0215, 137: 0.2571, 138: 0.0188, 139: 0.2848, 140: 0.0403, 141: 0.521, 142: -0.1123, 143: -0.4189, 144: -0.059, 145: 0.1411, 146: 0.6961, 147: -0.9315, 148: -0.4361, 149: -0.018, 150: 0.0119, 151: 0.1038, 152: -0.0026, 153: -0.2894, 154: -0.0158, 155: 0.3348, 156: 0.3371, 157: 0.184, 158: 0.0004, 159: 0.1126, 160: 2.1302, 161: 2.529, 162: -0.4205, 163: -1.2492, 164: 0.0398, 165: -0.5945, 166: 1.9414, 167: 1.6664, 168: 0.8844, 169: 0.3398, 170: 0.9885, 171: 0.9196, 172: 1.5143, 173: 0.152, 174: 0.3519, 175: 0.8515, 176: 0.2009, 177: 0.3299, 178: -0.0287, 179: 0.1306, 180: 0.2326, 181: -0.5418, 182: -0.3784, 183: -0.1563, 184: 0.4604, 185: 0.3369, 186: -0.0699, 187: -0.0277, 188: -0.0105, 189: 0.1575, 190: 0.0539, 191: -0.1763, 192: 0.1542, 193: 0.3117, 194: -0.2718, 195: -0.1575, 196: -0.1299, 197: -0.0834, 198: 0.0132, 199: 0.0275, 292: 5.4888, 300: 6372.0, 301: 5076.0, 302: 125.0, 303: 0.0246, 304: 565.0, 305: 360.0, 306: 0.6372, 597: 1.0}))]\n",
      "**********LogisticRegression**********\n",
      "accuracy  =  0.838501612503101\n",
      "f1  =  0.8403457610706984\n",
      "AUC = 0.9271063922014723\n",
      "time taken for LogisticRegression:  0:02:32.100225\n",
      "**********DecisionTreeClassifier**********\n",
      "accuracy  =  0.8079880922847928\n",
      "f1  =  0.8140101018586337\n",
      "AUC = 0.8772209168870959\n",
      "time taken for DecisionTreeClassifier:  0:02:29.322734\n",
      "**********RandomForestClassifier**********\n",
      "accuracy  =  0.838501612503101\n",
      "f1  =  0.8307857716778799\n",
      "AUC = 0.9346756119066335\n",
      "time taken for RandomForestClassifier:  0:05:31.507846\n",
      "**********GBTClassifier**********\n",
      "accuracy  =  0.9193748449516249\n",
      "f1  =  0.909952660581755\n",
      "AUC = 0.9649002773221464\n",
      "time taken for GBTClassifier:  0:24:28.691867\n",
      "**********MultilayerPerceptronClassifier**********\n",
      "accuracy  =  0.8342842967005706\n",
      "f1  =  0.8342257760201657\n",
      "AUC = 0.9241637235053922\n",
      "time taken for MultilayerPerceptronClassifier:  0:02:03.362296\n",
      "CPU times: user 1.57 s, sys: 622 ms, total: 2.19 s\n",
      "Wall time: 43min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"**********Run Models with PCA Features**********\")\n",
    "# generate the features to be used for model training\n",
    "preproc_model = build_data_preproc_model_with_pca(2000).fit(df_train)\n",
    "df_train_pca = preproc_model.transform(df_train).select(\"label\", \"features\")\n",
    "print(df_train_pca.take(1))\n",
    "df_test_pca = preproc_model.transform(df_test).select(\"label\", \"features\")\n",
    "print(df_test_pca.take(1))\n",
    "run_models(df_train_pca, df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Run Models without PCA Features**********\n",
      "[Row(label=1, features=SparseVector(4464, {0: 3213.0, 1: 2548.0, 2: 73.0, 3: 0.0286, 4: 304.0, 5: 243.0, 6: 0.7993, 2852: 1.0}))]\n",
      "[Row(label=1, features=SparseVector(4464, {0: 17072.0, 1: 13663.0, 2: 565.0, 3: 0.0414, 4: 1545.0, 5: 933.0, 6: 0.6039, 2022: 1.0}))]\n",
      "accuracy  =  0.9469114363681469\n",
      "f1  =  0.9456319370176287\n",
      "AUC = 0.9821325481619397\n",
      "CPU times: user 334 ms, sys: 48.1 ms, total: 382 ms\n",
      "Wall time: 3min 37s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "print(\"**********Run Models without PCA Features**********\")\n",
    "# generate the features to be used for model training\n",
    "preproc_model = build_data_preproc_model_without_pca(2000).fit(df_train)\n",
    "df_train_wo_pca = preproc_model.transform(df_train).select(\"label\", \"features\")\n",
    "print(df_train_wo_pca.take(1))\n",
    "df_test_wo_pca = preproc_model.transform(df_test).select(\"label\", \"features\")\n",
    "print(df_test_wo_pca.take(1))\n",
    "\n",
    "nb_model = NaiveBayes(featuresCol='features', \n",
    "                      labelCol='label', \n",
    "                      predictionCol='prediction', \n",
    "                      probabilityCol='probability', \n",
    "                      rawPredictionCol='rawPrediction', \n",
    "                      smoothing=1, \n",
    "                      modelType='multinomial')\n",
    "\n",
    "nb_model = nb_model.fit(df_train_wo_pca)\n",
    "eval_model_perf(nb_model, df_test_wo_pca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

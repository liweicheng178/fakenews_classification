{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Requirement already satisfied: gcsfs in /opt/conda/anaconda/lib/python3.6/site-packages (0.6.0)\nRequirement already satisfied: requests in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (2.23.0)\nRequirement already satisfied: google-auth>=1.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (1.11.3)\nRequirement already satisfied: decorator in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (4.4.2)\nRequirement already satisfied: google-auth-oauthlib in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (0.4.1)\nRequirement already satisfied: fsspec>=0.6.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (0.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (2019.11.28)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (2.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (1.25.8)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (3.0.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (0.2.8)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (1.14.0)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (4.0.0)\nRequirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (4.0)\nRequirement already satisfied: setuptools>=40.3.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (46.0.0.post20200309)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/anaconda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n"}], "source": "!pip install gcsfs"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "3.6.10\nCPU times: user 61.9 ms, sys: 12.2 ms, total: 74.1 ms\nWall time: 71.7 ms\n"}], "source": "%%time\nimport pandas as pd\nimport os, sys, time, json, re, string\nfrom google.cloud import storage\n\nfrom pyspark import SparkContext, SparkConf, StorageLevel, keyword_only\nfrom pyspark.sql.types import IntegerType\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.ml.linalg import Vectors, VectorUDT\nfrom pyspark.ml.param.shared import HasInputCol, HasInputCols, HasOutputCol, HasOutputCols, Param\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer, NGram, CountVectorizer, StopWordsRemover\nfrom pyspark.ml.feature import VectorAssembler, PCA\n\nfrom pyspark.ml.classification import LogisticRegression, NaiveBayes, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier\nfrom pyspark.ml import Pipeline, Transformer\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom platform import python_version\nprint(python_version())"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 3 \u00b5s, sys: 0 ns, total: 3 \u00b5s\nWall time: 5.96 \u00b5s\n"}, {"data": {"text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://cluster-b477-m.asia-southeast1-a.c.weicheng.internal:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.5</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        ", "text/plain": "<SparkContext master=yarn appName=PySparkShell>"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "%%time\nsc"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 6.37 ms, sys: 0 ns, total: 6.37 ms\nWall time: 9.38 ms\n"}], "source": "%%time\nspark = SparkSession.builder \\\n        .appName(\"fakenews_baseline_model\") \\\n        .config(\"spark.master\", \"yarn\") \\\n        .config(\"spark.submit.deployMode\", \"cluster\") \\\n        .config(\"spark.driver.memory\", \"24g\") \\\n        .config(\"spark.executor.instances\", \"5\") \\\n        .config(\"spark.executor.cores\", \"4\") \\\n        .config(\"spark.executor.memory\", \"24g\") \\\n        .getOrCreate()"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 217806 entries, 0 to 108010\nData columns (total 7 columns):\ndomain             217806 non-null object\ntype               217806 non-null object\ncontent            217806 non-null object\ntitle              217806 non-null object\nauthors            217806 non-null object\nauthors_missing    217806 non-null object\nlabel              217806 non-null object\ndtypes: object(7)\nmemory usage: 13.3+ MB\nNone\nCPU times: user 9.11 s, sys: 1.08 s, total: 10.2 s\nWall time: 20.7 s\n"}, {"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>domain</th>\n      <th>type</th>\n      <th>content</th>\n      <th>title</th>\n      <th>authors</th>\n      <th>authors_missing</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>beforeitsnews.com</td>\n      <td>fake</td>\n      <td>Boehner presses to make tax cuts permanent\\n\\n...</td>\n      <td>Boehner presses to make tax cuts permanent</td>\n      <td>United Liberty</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>beforeitsnews.com</td>\n      <td>fake</td>\n      <td>An Armed Good Samaritan Who Doesn\u2019t Want The S...</td>\n      <td>An Armed Good Samaritan Who Doesn\u2019t Want The S...</td>\n      <td>The Real Revo</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beforeitsnews.com</td>\n      <td>fake</td>\n      <td>(Before It's News)\\n\\nby Rob Morphy\\n\\nLegends...</td>\n      <td>Village Of The Dead: The Anjikuni Mystery</td>\n      <td>Rob Morphy, Mort Amsel</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>teaparty.org</td>\n      <td>fake</td>\n      <td>(Breitbart) \u2013 With his hysterical gotcha attac...</td>\n      <td>Trump Calls Out Race-Baiting ABC News Reporter</td>\n      <td>nan</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>beforeitsnews.com</td>\n      <td>fake</td>\n      <td>Researchers develop new depression diagnosis a...</td>\n      <td>Researchers develop new depression diagnosis a...</td>\n      <td>Bel Marra Health</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>108006</th>\n      <td>sports.yahoo.com</td>\n      <td>reliable</td>\n      <td>View photos\\nMichigan guard Zak Irvin, right, ...</td>\n      <td>No. 25 Michigan beats Mount St. Mary's 64-47</td>\n      <td>The Associated Press</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>108007</th>\n      <td>uk.finance.yahoo.com</td>\n      <td>reliable</td>\n      <td>President-elect Donald Trump continued his scr...</td>\n      <td>Trump quotes Hillary Clinton, rages against Wi...</td>\n      <td>Maxwell Tani</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>108008</th>\n      <td>www.yahoo.com</td>\n      <td>reliable</td>\n      <td>Holiday shoppers eager to snag big discounts t...</td>\n      <td>Dialing up deals: Black Friday online sales hi...</td>\n      <td>ALEX VEIGA</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>108009</th>\n      <td>www.reuters.com</td>\n      <td>reliable</td>\n      <td>Kabul police raid shisha cafes in crackdown on...</td>\n      <td>Kabul police raid shisha cafes in crackdown on...</td>\n      <td>nan</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>108010</th>\n      <td>m.mlb.com</td>\n      <td>reliable</td>\n      <td>2016-17 MLB.com Hot Stove \\n\u00a9 2001- 2016 MLB A...</td>\n      <td>Possibility of Granderson in CF | MLB.com</td>\n      <td>nan</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>217806 rows \u00d7 7 columns</p>\n</div>", "text/plain": "                      domain      type  \\\n0          beforeitsnews.com      fake   \n1          beforeitsnews.com      fake   \n2          beforeitsnews.com      fake   \n3               teaparty.org      fake   \n4          beforeitsnews.com      fake   \n...                      ...       ...   \n108006      sports.yahoo.com  reliable   \n108007  uk.finance.yahoo.com  reliable   \n108008         www.yahoo.com  reliable   \n108009       www.reuters.com  reliable   \n108010             m.mlb.com  reliable   \n\n                                                  content  \\\n0       Boehner presses to make tax cuts permanent\\n\\n...   \n1       An Armed Good Samaritan Who Doesn\u2019t Want The S...   \n2       (Before It's News)\\n\\nby Rob Morphy\\n\\nLegends...   \n3       (Breitbart) \u2013 With his hysterical gotcha attac...   \n4       Researchers develop new depression diagnosis a...   \n...                                                   ...   \n108006  View photos\\nMichigan guard Zak Irvin, right, ...   \n108007  President-elect Donald Trump continued his scr...   \n108008  Holiday shoppers eager to snag big discounts t...   \n108009  Kabul police raid shisha cafes in crackdown on...   \n108010  2016-17 MLB.com Hot Stove \\n\u00a9 2001- 2016 MLB A...   \n\n                                                    title  \\\n0              Boehner presses to make tax cuts permanent   \n1       An Armed Good Samaritan Who Doesn\u2019t Want The S...   \n2               Village Of The Dead: The Anjikuni Mystery   \n3          Trump Calls Out Race-Baiting ABC News Reporter   \n4       Researchers develop new depression diagnosis a...   \n...                                                   ...   \n108006       No. 25 Michigan beats Mount St. Mary's 64-47   \n108007  Trump quotes Hillary Clinton, rages against Wi...   \n108008  Dialing up deals: Black Friday online sales hi...   \n108009  Kabul police raid shisha cafes in crackdown on...   \n108010          Possibility of Granderson in CF | MLB.com   \n\n                       authors authors_missing label  \n0               United Liberty               0     1  \n1                The Real Revo               0     1  \n2       Rob Morphy, Mort Amsel               0     1  \n3                          nan               1     1  \n4             Bel Marra Health               0     1  \n...                        ...             ...   ...  \n108006    The Associated Press               0     0  \n108007            Maxwell Tani               0     0  \n108008              ALEX VEIGA               0     0  \n108009                     nan               1     0  \n108010                     nan               1     0  \n\n[217806 rows x 7 columns]"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "%%time\ndf_fake_train = pd.read_csv('gs://dataproc-6ca41800-27b4-47d5-abee-55c011dfa389-asia-southeast1/data/fake-news/100k_fake_news_cleaned_dataset.csv')\ndf_fake_train[['authors_missing']] = df_fake_train[['authors_missing']].astype(int)\ndf_fake_train['label'] = 1\ndf_reliable_train = pd.read_csv('gs://dataproc-6ca41800-27b4-47d5-abee-55c011dfa389-asia-southeast1/data/fake-news/100k_reliable_news_cleaned_dataset.csv')\ndf_reliable_train[['authors_missing']] = df_reliable_train[['authors_missing']].astype(int)\ndf_reliable_train['label'] = 0\ndf_news_train = pd.concat([df_fake_train, df_reliable_train])\ndf_news_train[df_news_train.columns] = df_news_train[df_news_train.columns].astype(str)\nprint(df_news_train.info())\ndf_news_train"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 13.1 s, sys: 776 ms, total: 13.8 s\nWall time: 15 s\n"}], "source": "%%time\ndf_news = spark.createDataFrame(df_news_train)\ndf_news = df_news.withColumn(\"label\", df_news[\"label\"].cast(IntegerType()))\ndf_news = df_news.withColumn(\"authors_missing\", df_news[\"authors_missing\"].cast(IntegerType()))"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- domain: string (nullable = true)\n |-- type: string (nullable = true)\n |-- content: string (nullable = true)\n |-- title: string (nullable = true)\n |-- authors: string (nullable = true)\n |-- authors_missing: integer (nullable = true)\n |-- label: integer (nullable = true)\n\nNone\n"}], "source": "print(df_news.printSchema())"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+----+--------------------+--------------------+--------------------+---------------+-----+\n|           domain|type|             content|               title|             authors|authors_missing|label|\n+-----------------+----+--------------------+--------------------+--------------------+---------------+-----+\n|beforeitsnews.com|fake|Boehner presses t...|Boehner presses t...|      United Liberty|              0|    1|\n|beforeitsnews.com|fake|An Armed Good Sam...|An Armed Good Sam...|       The Real Revo|              0|    1|\n|beforeitsnews.com|fake|(Before It's News...|Village Of The De...|Rob Morphy, Mort ...|              0|    1|\n|     teaparty.org|fake|(Breitbart) \u2013 Wit...|Trump Calls Out R...|                 nan|              1|    1|\n|beforeitsnews.com|fake|Researchers devel...|Researchers devel...|    Bel Marra Health|              0|    1|\n|beforeitsnews.com|fake|Trading Watch Lis...|Trading Watch Lis...|Bulls On Wall Street|              0|    1|\n|beforeitsnews.com|fake|Looks like today ...|Looks like today ...|      Protein Wisdom|              0|    1|\n|beforeitsnews.com|fake|On the paradox of...|On the paradox of...|The Adam Smith In...|              0|    1|\n|beforeitsnews.com|fake|% of readers thin...|Minuteman- Most I...|         Nc Renegade|              0|    1|\n|beforeitsnews.com|fake|Juniper to Trim E...|Juniper to Trim E...|Zacks Investment ...|              0|    1|\n+-----------------+----+--------------------+--------------------+--------------------+---------------+-----+\nonly showing top 10 rows\n\n"}], "source": "df_news.show(10)"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 8.12 ms, sys: 8 \u00b5s, total: 8.12 ms\nWall time: 410 ms\n"}], "source": "%%time\n# only keep type and content\ndf_news = df_news.select(\"label\", \"content\")\n\n#remove empty content which will cause problem when transform the text\ndf_news = df_news.filter(df_news.content != \"\")\n\n# split the dataset\ndf_train, df_test = df_news.randomSplit([0.8, 0.2], seed=666)\nparam_tuning = False"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 106 \u00b5s, sys: 9 \u00b5s, total: 115 \u00b5s\nWall time: 121 \u00b5s\n"}], "source": "%%time\n# customized transformer class to manually extract some counting based text features\nclass ReviewContentTransformer(Transformer, HasInputCol, HasOutputCol):\n\n    @keyword_only\n    def __init__(self, inputCol=\"content\", outputCol=\"content_features\"):\n        super(ReviewContentTransformer, self).__init__()\n        kwargs = self._input_kwargs\n        self.setParams(**kwargs)\n\n    @keyword_only\n    def setParams(self, inputCol=None, outputCol=None):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n\n    def _transform(self, dataset):\n        \n        def f(s):\n            uppercase_count = 0\n            char_count = 0\n            for c in s:                \n                if c in string.ascii_uppercase:\n                    uppercase_count += 1\n                    char_count += 1\n                elif c in string.ascii_lowercase:\n                    char_count += 1\n            \n            text_len = len(s)\n            return Vectors.dense(text_len, char_count, \n                                 uppercase_count, uppercase_count / (char_count + 1e-10))\n\n        return dataset.withColumn(self.getOutputCol(), \n                                  F.udf(f, VectorUDT())(dataset[self.getInputCol()]))"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 110 \u00b5s, sys: 0 ns, total: 110 \u00b5s\nWall time: 116 \u00b5s\n"}], "source": "%%time\n# customized transformer class to manually extract some counting based word features\nclass ReviewWordsTransformer(Transformer, HasInputCol, HasOutputCol):\n\n    @keyword_only\n    def __init__(self, inputCol=\"content\", outputCol=\"content_features\"):\n        super(ReviewWordsTransformer, self).__init__()\n        kwargs = self._input_kwargs\n        self.setParams(**kwargs)\n\n    @keyword_only\n    def setParams(self, inputCol=None, outputCol=None):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n\n    def _transform(self, dataset):\n        \n        def f(words):    \n            word_count = len(words)\n            unique_word_count = len(set(words))\n            upper_words = []\n            for w in words:\n                if w.isupper():\n                    upper_words.append(w)\n            upper_word_count = len(set(upper_words))\n            unique_upper_word_count = len(upper_words)\n            return Vectors.dense(word_count, unique_word_count, unique_word_count / (word_count + 1e-10),\n                                 upper_word_count, upper_word_count / (word_count + 1e-10), \n                                 unique_upper_word_count, unique_upper_word_count / (upper_word_count + 1e-10))\n\n        return dataset.withColumn(self.getOutputCol(), \n                                  F.udf(f, VectorUDT())(dataset[self.getInputCol()]))"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 9 \u00b5s, sys: 1e+03 ns, total: 10 \u00b5s\nWall time: 16.5 \u00b5s\n"}], "source": "%%time\n# show model prediction performance on the given dataset\ndef eval_model_perf(fitted_model, dataset, label_col=\"label\", prediction_col=\"prediction\", probability_col=\"probability\"):\n    pred_dataset = fitted_model.transform(dataset)\n    eval_dataset = pred_dataset.select(label_col, prediction_col, probability_col)\n    # model performance evaluation\n    metricNames = [\"accuracy\", \"f1\"]\n    model_eval = MulticlassClassificationEvaluator(predictionCol=prediction_col, labelCol=label_col)\n    for m in metricNames:\n        val = model_eval.evaluate(eval_dataset, {model_eval.metricName: m})\n        print(m, \" = \", val)\n    roc_eval = BinaryClassificationEvaluator(rawPredictionCol=probability_col, labelCol=label_col, metricName=\"areaUnderROC\")\n    print(\"AUC =\", roc_eval.evaluate(eval_dataset))    \n    return pred_dataset\n\n# show CV param tunning result\ndef show_cv_results(cv_model):\n    for result, param in sorted(zip(cv_model.avgMetrics, cv_model.getEstimatorParamMaps()), reverse=True, key=lambda x: x[0]):\n        print(result, \" | \", param)"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 7 \u00b5s, sys: 0 ns, total: 7 \u00b5s\nWall time: 11.7 \u00b5s\n"}], "source": "%%time\ndef run_models(df_train, df_test):\n    print(\"**********LogisticRegression**********\")\n    t = time.time()\n    lr_model = LogisticRegression(featuresCol='features', \n                                  labelCol='label', \n                                  predictionCol='prediction', \n                                  probabilityCol='probability', \n                                  rawPredictionCol='rawPrediction',\n                                  family='binomial', \n                                  fitIntercept=True, \n                                  threshold=0.5, \n                                  standardization=False, \n                                  maxIter=200, \n                                  regParam=0.005, \n                                  elasticNetParam=0, \n                                  tol=1e-06, \n                                  aggregationDepth=2)\n\n    lr_model = lr_model.fit(df_train)\n    \n    eval_model_perf(lr_model, df_test)\n    \n    print(\"time taken for LogisticRegression: \", time.time() - t)\n    t = time.time()\n\n    print(\"**********DecisionTreeClassifier**********\")\n    dt_model = DecisionTreeClassifier(featuresCol='features', \n                                      labelCol='label', \n                                      predictionCol='prediction', \n                                      probabilityCol='probability', \n                                      rawPredictionCol='rawPrediction', \n                                      maxDepth=10, maxBins=32, \n                                      minInstancesPerNode=1, \n                                      minInfoGain=0.0, \n                                      maxMemoryInMB=2048, \n                                      cacheNodeIds=True, \n                                      checkpointInterval=10,\n                                      impurity='gini', \n                                      seed=666)\n\n    dt_model = dt_model.fit(df_train)\n    eval_model_perf(dt_model, df_test)\n    print(\"time taken for DecisionTreeClassifier: \", time.time() - t)\n    t = time.time()\n    \n    print(\"**********RandomForestClassifier**********\")\n    rf_model = RandomForestClassifier(featuresCol='features', \n                                      labelCol='label', \n                                      predictionCol='prediction', \n                                      probabilityCol='probability', \n                                      rawPredictionCol='rawPrediction',\n                                      maxDepth=10, \n                                      maxBins=32, \n                                      minInstancesPerNode=1, \n                                      minInfoGain=0.0, \n                                      maxMemoryInMB=2048, \n                                      cacheNodeIds=True, \n                                      checkpointInterval=10, \n                                      impurity='gini', \n                                      numTrees=200, \n                                      featureSubsetStrategy='auto', \n                                      seed=666, \n                                      subsamplingRate=0.8)\n\n    rf_model = rf_model.fit(df_train)\n    eval_model_perf(rf_model, df_test)\n    print(\"time taken for RandomForestClassifier: \", time.time() - t)\n    t = time.time()\n    \n    print(\"**********GBTClassifier**********\")\n    gbt_model = GBTClassifier(featuresCol='features', \n                             labelCol='label', \n                             maxIter=250)\n\n    gbt_model = gbt_model.fit(df_train)\n    eval_model_perf(gbt_model, df_test)\n    print(\"time taken for GBTClassifier: \", time.time() - t)\n    t = time.time()    \n    \n    print(\"**********MultilayerPerceptronClassifier**********\")\n    mp_model = MultilayerPerceptronClassifier(featuresCol='features', \n                                              labelCol='label', \n                                              predictionCol='prediction', \n                                              layers=[4, 5, 4, 3],  \n                                              maxIter=100, \n                                              blockSize=128, \n                                              seed=1234)\n\n    mp_model = mp_model.fit(df_train)\n    eval_model_perf(rf_model, df_test)\n    print(\"time taken for MultilayerPerceptronClassifier: \", time.time() - t)\n    t = time.time()  "}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 7 \u00b5s, sys: 1e+03 ns, total: 8 \u00b5s\nWall time: 11.2 \u00b5s\n"}], "source": "%%time\ndef build_data_preproc_model_with_pca(vocab_size=5000):\n    preproc_steps = [\n        RegexTokenizer(inputCol=\"content\", outputCol=\"all_words\", pattern=r\"\\W\"),\n        StopWordsRemover(inputCol=\"all_words\", outputCol=\"words\"),\n        CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=vocab_size),\n        IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\"),\n        PCA(inputCol=\"tfidf_features\", outputCol=\"pca_features\", k=100),\n        ReviewContentTransformer(inputCol=\"content\", outputCol=\"content_features\"),\n        ReviewWordsTransformer(inputCol=\"words\", outputCol=\"word_features\"),\n        VectorAssembler(inputCols=[\"pca_features\", \"content_features\", \"word_features\"], \n                        outputCol=\"features\")\n    ]\n    return Pipeline(stages=preproc_steps)\n\ndef build_data_preproc_model_without_pca(vocab_size=5000):\n    preproc_steps = [\n        RegexTokenizer(inputCol=\"content\", outputCol=\"all_words\", pattern=r\"\\W\"),\n        StopWordsRemover(inputCol=\"all_words\", outputCol=\"words\"),\n        CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=vocab_size),\n        IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\"),\n        ReviewContentTransformer(inputCol=\"content\", outputCol=\"content_features\"),\n        ReviewWordsTransformer(inputCol=\"words\", outputCol=\"word_features\"),\n        VectorAssembler(inputCols=[\"content_features\", \"word_features\"], outputCol=\"features\")\n    ]\n    return Pipeline(stages=preproc_steps)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "**********Run Models with PCA Features**********\n[Row(label=1, features=DenseVector([-22.0074, 1.7468, 10.5792, 4.9642, 11.3049, -3.0668, -8.332, -1.4447, -15.7509, -1.6806, 2.1436, 1.964, -4.3619, -1.677, -6.9755, -6.0372, 5.4687, -1.7859, -9.5688, 4.0396, 2.7192, -3.603, 3.0286, 5.9583, 0.2917, -8.276, 6.01, -1.843, 2.0768, 1.5102, -0.296, 2.976, 0.2103, 0.6911, 1.2656, 0.3281, 4.053, 3.1602, -2.5984, -1.4353, 4.966, -2.1797, 0.6323, 4.3106, 1.7353, -1.0416, -0.2837, 0.1344, -1.5166, 2.5739, -3.4595, 1.2579, 0.9333, -0.1843, 3.0886, 0.6023, -3.3606, 5.5071, -1.1863, 4.3941, 0.9795, -1.2223, -4.2016, -3.9709, -1.2527, 0.9829, -5.5237, -4.6546, 3.8882, -1.2162, 6.8242, -0.8683, 5.1257, -5.8941, -3.0483, 3.852, -0.6034, 1.4498, 4.4992, -1.7428, 3.627, 0.9651, 4.1241, -1.9163, 3.9631, 3.0485, -3.5137, -1.2811, 4.7492, 3.6088, 0.6963, -0.5498, -2.2639, -2.45, -5.0489, 0.4113, 0.3066, -1.899, -2.7421, -1.5389, 6479.0, 5136.0, 179.0, 0.0349, 618.0, 430.0, 0.6958, 0.0, 0.0, 0.0, 0.0]))]\n[Row(label=1, features=DenseVector([-33.6313, 0.4101, 12.8463, 2.8891, 15.5411, 3.6346, -12.8782, 0.2628, -22.8614, -3.9639, 2.9092, -3.447, -2.273, -0.4881, 4.7774, -6.1725, -6.6003, -6.7875, -4.9777, 1.0661, -1.5023, 5.6243, 3.0485, 2.4493, 0.4251, 5.0195, -0.108, 3.6706, -0.8098, 2.55, -2.1591, 0.8713, -1.9283, -5.4269, -1.1253, -6.2446, 2.0507, 4.517, -7.637, -6.3974, -4.3164, 1.5748, -0.9963, 3.5509, -4.6608, -3.2128, -2.2286, -3.9473, -4.5706, -2.4909, -1.953, 3.9367, 7.7259, -6.8222, 1.7915, 3.4012, -3.046, 2.9504, -7.8306, 3.0399, -2.5588, 3.0912, -3.6259, -1.8226, 0.4088, -6.9087, 3.6412, -5.6405, 1.8422, -8.0587, -0.7594, 0.0677, -0.1349, 0.2924, 1.0221, -8.0399, 1.29, -0.0572, -1.3121, 4.3124, 3.1119, -6.2019, -6.2749, 2.5142, 5.7169, 7.4778, -7.3701, -4.3208, -1.9411, 5.9413, 7.3421, 2.3303, 1.7021, 5.4685, -1.3567, -5.8351, 4.9603, -0.6175, -6.4389, -6.2549, 13886.0, 11027.0, 396.0, 0.0359, 1282.0, 802.0, 0.6256, 0.0, 0.0, 0.0, 0.0]))]\n**********LogisticRegression**********\naccuracy  =  0.8887289181406829\nf1  =  0.8886265676854614\nAUC = 0.9493681432319702\ntime taken for LogisticRegression:  763.8673977851868\n**********DecisionTreeClassifier**********\naccuracy  =  0.83347045111751\nf1  =  0.833457702218225\nAUC = 0.9010306313986896\ntime taken for DecisionTreeClassifier:  318.9003207683563\n**********RandomForestClassifier**********\naccuracy  =  0.8702180172768408\nf1  =  0.8701991847597366\nAUC = 0.9465821555655272\ntime taken for RandomForestClassifier:  401.6509232521057\n**********GBTClassifier**********\naccuracy  =  0.902829196946844\nf1  =  0.9028275955587617\nAUC = 0.9670647724389898\ntime taken for GBTClassifier:  5611.331223726273\n**********MultilayerPerceptronClassifier**********\naccuracy  =  0.8702180172768408\nf1  =  0.8701991847597366\nAUC = 0.9465821555655257\ntime taken for MultilayerPerceptronClassifier:  499.282990694046\nCPU times: user 2.61 s, sys: 859 ms, total: 3.47 s\nWall time: 2h 8min 41s\n"}], "source": "%%time\nprint(\"**********Run Models with PCA Features**********\")\n# generate the features to be used for model training\npreproc_model = build_data_preproc_model_with_pca(3000).fit(df_train)\ndf_train_pca = preproc_model.transform(df_train).select(\"label\", \"features\")\nprint(df_train_pca.take(1))\ndf_test_pca = preproc_model.transform(df_test).select(\"label\", \"features\")\nprint(df_test_pca.take(1))\nrun_models(df_train_pca, df_test_pca)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "**********Run Models without PCA Features**********\n[Row(label=1, features=DenseVector([6479.0, 5136.0, 179.0, 0.0349, 618.0, 430.0, 0.6958, 0.0, 0.0, 0.0, 0.0]))]\n[Row(label=1, features=DenseVector([13886.0, 11027.0, 396.0, 0.0359, 1282.0, 802.0, 0.6256, 0.0, 0.0, 0.0, 0.0]))]\naccuracy  =  0.5704556881027469\nf1  =  0.5665654197967869\nAUC = 0.6240900787267446\nCPU times: user 248 ms, sys: 49.5 ms, total: 297 ms\nWall time: 3min 11s\n"}, {"data": {"text/plain": "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}], "source": "%%time\nprint(\"**********Run Models without PCA Features**********\")\n# generate the features to be used for model training\npreproc_model = build_data_preproc_model_without_pca(3000).fit(df_train)\ndf_train_wo_pca = preproc_model.transform(df_train).select(\"label\", \"features\")\nprint(df_train_wo_pca.take(1))\ndf_test_wo_pca = preproc_model.transform(df_test).select(\"label\", \"features\")\nprint(df_test_wo_pca.take(1))\n\nnb_model = NaiveBayes(featuresCol='features', \n                      labelCol='label', \n                      predictionCol='prediction', \n                      probabilityCol='probability', \n                      rawPredictionCol='rawPrediction', \n                      smoothing=1, \n                      modelType='multinomial')\n\nnb_model = nb_model.fit(df_train_wo_pca)\neval_model_perf(nb_model, df_test_wo_pca)"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.10"}}, "nbformat": 4, "nbformat_minor": 2}
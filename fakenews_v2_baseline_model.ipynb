{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /opt/conda/anaconda/lib/python3.6/site-packages (20.0.2)\n",
      "Requirement already satisfied: gcsfs in /opt/conda/anaconda/lib/python3.6/site-packages (0.6.1)\n",
      "Requirement already satisfied: requests in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (2.23.0)\n",
      "Requirement already satisfied: google-auth>=1.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (1.11.3)\n",
      "Requirement already satisfied: decorator in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (4.4.2)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (0.4.1)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (0.7.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (3.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (1.14.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (4.0.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (4.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (46.0.0.post20200309)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/anaconda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.10\n",
      "CPU times: user 281 µs, sys: 0 ns, total: 281 µs\n",
      "Wall time: 239 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import os, sys, time, json, re, string\n",
    "from google.cloud import storage\n",
    "\n",
    "from pyspark import SparkContext, SparkConf, StorageLevel, keyword_only\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.param.shared import HasInputCol, HasInputCols, HasOutputCol, HasOutputCols, Param\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer, NGram, CountVectorizer, StopWordsRemover\n",
    "from pyspark.ml.feature import VectorAssembler, PCA\n",
    "\n",
    "from pyspark.ml.classification import * \n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-8544-m.asia-southeast1-a.c.weicheng-273112.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.04 ms, sys: 932 µs, total: 6.97 ms\n",
      "Wall time: 10.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"fakenews\") \\\n",
    "        .config(\"spark.master\", \"yarn\") \\\n",
    "        .config(\"spark.submit.deployMode\", \"cluster\") \\\n",
    "        .config(\"spark.driver.memory\", \"25g\") \\\n",
    "        .config(\"spark.executor.instances\", \"5\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.executor.memory\", \"25g\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 217806 entries, 0 to 108010\n",
      "Data columns (total 7 columns):\n",
      "domain             217806 non-null object\n",
      "type               217806 non-null object\n",
      "content            217806 non-null object\n",
      "title              217806 non-null object\n",
      "authors            217806 non-null object\n",
      "authors_missing    217806 non-null object\n",
      "label              217806 non-null object\n",
      "dtypes: object(7)\n",
      "memory usage: 13.3+ MB\n",
      "None\n",
      "CPU times: user 9.54 s, sys: 1.85 s, total: 11.4 s\n",
      "Wall time: 23.6 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>authors_missing</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>Boehner presses to make tax cuts permanent\\n\\n...</td>\n",
       "      <td>Boehner presses to make tax cuts permanent</td>\n",
       "      <td>United Liberty</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>An Armed Good Samaritan Who Doesn’t Want The S...</td>\n",
       "      <td>An Armed Good Samaritan Who Doesn’t Want The S...</td>\n",
       "      <td>The Real Revo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>(Before It's News)\\n\\nby Rob Morphy\\n\\nLegends...</td>\n",
       "      <td>Village Of The Dead: The Anjikuni Mystery</td>\n",
       "      <td>Rob Morphy, Mort Amsel</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>teaparty.org</td>\n",
       "      <td>fake</td>\n",
       "      <td>(Breitbart) – With his hysterical gotcha attac...</td>\n",
       "      <td>Trump Calls Out Race-Baiting ABC News Reporter</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>Researchers develop new depression diagnosis a...</td>\n",
       "      <td>Researchers develop new depression diagnosis a...</td>\n",
       "      <td>Bel Marra Health</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108006</th>\n",
       "      <td>sports.yahoo.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>View photos\\nMichigan guard Zak Irvin, right, ...</td>\n",
       "      <td>No. 25 Michigan beats Mount St. Mary's 64-47</td>\n",
       "      <td>The Associated Press</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108007</th>\n",
       "      <td>uk.finance.yahoo.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>President-elect Donald Trump continued his scr...</td>\n",
       "      <td>Trump quotes Hillary Clinton, rages against Wi...</td>\n",
       "      <td>Maxwell Tani</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108008</th>\n",
       "      <td>www.yahoo.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>Holiday shoppers eager to snag big discounts t...</td>\n",
       "      <td>Dialing up deals: Black Friday online sales hi...</td>\n",
       "      <td>ALEX VEIGA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108009</th>\n",
       "      <td>www.reuters.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>Kabul police raid shisha cafes in crackdown on...</td>\n",
       "      <td>Kabul police raid shisha cafes in crackdown on...</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108010</th>\n",
       "      <td>m.mlb.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>2016-17 MLB.com Hot Stove \\n© 2001- 2016 MLB A...</td>\n",
       "      <td>Possibility of Granderson in CF | MLB.com</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>217806 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      domain      type  \\\n",
       "0          beforeitsnews.com      fake   \n",
       "1          beforeitsnews.com      fake   \n",
       "2          beforeitsnews.com      fake   \n",
       "3               teaparty.org      fake   \n",
       "4          beforeitsnews.com      fake   \n",
       "...                      ...       ...   \n",
       "108006      sports.yahoo.com  reliable   \n",
       "108007  uk.finance.yahoo.com  reliable   \n",
       "108008         www.yahoo.com  reliable   \n",
       "108009       www.reuters.com  reliable   \n",
       "108010             m.mlb.com  reliable   \n",
       "\n",
       "                                                  content  \\\n",
       "0       Boehner presses to make tax cuts permanent\\n\\n...   \n",
       "1       An Armed Good Samaritan Who Doesn’t Want The S...   \n",
       "2       (Before It's News)\\n\\nby Rob Morphy\\n\\nLegends...   \n",
       "3       (Breitbart) – With his hysterical gotcha attac...   \n",
       "4       Researchers develop new depression diagnosis a...   \n",
       "...                                                   ...   \n",
       "108006  View photos\\nMichigan guard Zak Irvin, right, ...   \n",
       "108007  President-elect Donald Trump continued his scr...   \n",
       "108008  Holiday shoppers eager to snag big discounts t...   \n",
       "108009  Kabul police raid shisha cafes in crackdown on...   \n",
       "108010  2016-17 MLB.com Hot Stove \\n© 2001- 2016 MLB A...   \n",
       "\n",
       "                                                    title  \\\n",
       "0              Boehner presses to make tax cuts permanent   \n",
       "1       An Armed Good Samaritan Who Doesn’t Want The S...   \n",
       "2               Village Of The Dead: The Anjikuni Mystery   \n",
       "3          Trump Calls Out Race-Baiting ABC News Reporter   \n",
       "4       Researchers develop new depression diagnosis a...   \n",
       "...                                                   ...   \n",
       "108006       No. 25 Michigan beats Mount St. Mary's 64-47   \n",
       "108007  Trump quotes Hillary Clinton, rages against Wi...   \n",
       "108008  Dialing up deals: Black Friday online sales hi...   \n",
       "108009  Kabul police raid shisha cafes in crackdown on...   \n",
       "108010          Possibility of Granderson in CF | MLB.com   \n",
       "\n",
       "                       authors authors_missing label  \n",
       "0               United Liberty               0     1  \n",
       "1                The Real Revo               0     1  \n",
       "2       Rob Morphy, Mort Amsel               0     1  \n",
       "3                          nan               1     1  \n",
       "4             Bel Marra Health               0     1  \n",
       "...                        ...             ...   ...  \n",
       "108006    The Associated Press               0     0  \n",
       "108007            Maxwell Tani               0     0  \n",
       "108008              ALEX VEIGA               0     0  \n",
       "108009                     nan               1     0  \n",
       "108010                     nan               1     0  \n",
       "\n",
       "[217806 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_fake_train = pd.read_csv('gs://weicheng30417/data/fake-news/100k_fake_news_cleaned_dataset.csv')\n",
    "df_fake_train[['authors_missing']] = df_fake_train[['authors_missing']].astype(int)\n",
    "df_fake_train['label'] = 1\n",
    "df_reliable_train = pd.read_csv('gs://weicheng30417/data/fake-news/100k_reliable_news_cleaned_dataset.csv')\n",
    "df_reliable_train[['authors_missing']] = df_reliable_train[['authors_missing']].astype(int)\n",
    "df_reliable_train['label'] = 0\n",
    "df_news_train = pd.concat([df_fake_train, df_reliable_train])\n",
    "df_news_train[df_news_train.columns] = df_news_train[df_news_train.columns].astype(str)\n",
    "print(df_news_train.info())\n",
    "df_news_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.6 s, sys: 1.12 s, total: 14.7 s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_news = spark.createDataFrame(df_news_train)\n",
    "df_news = df_news.withColumn(\"label\", df_news[\"label\"].cast(IntegerType()))\n",
    "df_news = df_news.withColumn(\"authors_missing\", df_news[\"authors_missing\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- domain: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- authors_missing: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_news.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+--------------------+--------------------+--------------------+---------------+-----+\n",
      "|           domain|type|             content|               title|             authors|authors_missing|label|\n",
      "+-----------------+----+--------------------+--------------------+--------------------+---------------+-----+\n",
      "|beforeitsnews.com|fake|Boehner presses t...|Boehner presses t...|      United Liberty|              0|    1|\n",
      "|beforeitsnews.com|fake|An Armed Good Sam...|An Armed Good Sam...|       The Real Revo|              0|    1|\n",
      "|beforeitsnews.com|fake|(Before It's News...|Village Of The De...|Rob Morphy, Mort ...|              0|    1|\n",
      "|     teaparty.org|fake|(Breitbart) – Wit...|Trump Calls Out R...|                 nan|              1|    1|\n",
      "|beforeitsnews.com|fake|Researchers devel...|Researchers devel...|    Bel Marra Health|              0|    1|\n",
      "|beforeitsnews.com|fake|Trading Watch Lis...|Trading Watch Lis...|Bulls On Wall Street|              0|    1|\n",
      "|beforeitsnews.com|fake|Looks like today ...|Looks like today ...|      Protein Wisdom|              0|    1|\n",
      "|beforeitsnews.com|fake|On the paradox of...|On the paradox of...|The Adam Smith In...|              0|    1|\n",
      "|beforeitsnews.com|fake|% of readers thin...|Minuteman- Most I...|         Nc Renegade|              0|    1|\n",
      "|beforeitsnews.com|fake|Juniper to Trim E...|Juniper to Trim E...|Zacks Investment ...|              0|    1|\n",
      "+-----------------+----+--------------------+--------------------+--------------------+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_news.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.54 ms, sys: 591 µs, total: 9.14 ms\n",
      "Wall time: 463 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# only keep type and content\n",
    "df_news = df_news.select(\"label\", \"content\")\n",
    "\n",
    "#remove empty content which will cause problem when transform the text\n",
    "df_news = df_news.filter(df_news.content != \"\")\n",
    "\n",
    "# split the dataset\n",
    "df_train, df_test = df_news.randomSplit([0.8, 0.2], seed=666)\n",
    "param_tuning = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 108 µs, sys: 14 µs, total: 122 µs\n",
      "Wall time: 127 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# customized transformer class to manually extract some counting based text features\n",
    "class ReviewContentTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=\"content\", outputCol=\"content_features\"):\n",
    "        super(ReviewContentTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        def f(s):\n",
    "            uppercase_count = 0\n",
    "            char_count = 0\n",
    "            for c in s:                \n",
    "                if c in string.ascii_uppercase:\n",
    "                    uppercase_count += 1\n",
    "                    char_count += 1\n",
    "                elif c in string.ascii_lowercase:\n",
    "                    char_count += 1\n",
    "            \n",
    "            text_len = len(s)\n",
    "            return Vectors.dense(text_len, char_count, \n",
    "                                 uppercase_count, uppercase_count / (char_count + 1e-10))\n",
    "\n",
    "        return dataset.withColumn(self.getOutputCol(), \n",
    "                                  F.udf(f, VectorUDT())(dataset[self.getInputCol()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 88 µs, sys: 12 µs, total: 100 µs\n",
      "Wall time: 103 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# customized transformer class to manually extract some counting based word features\n",
    "class ReviewWordsTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=\"content\", outputCol=\"content_features\"):\n",
    "        super(ReviewWordsTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        def f(words):    \n",
    "            word_count = len(words)\n",
    "            unique_word_count = len(set(words))\n",
    "            upper_words = []\n",
    "            for w in words:\n",
    "                if w.isupper():\n",
    "                    upper_words.append(w)\n",
    "            upper_word_count = len(set(upper_words))\n",
    "            unique_upper_word_count = len(upper_words)\n",
    "            return Vectors.dense(word_count, unique_word_count, unique_word_count / (word_count + 1e-10),\n",
    "                                 upper_word_count, upper_word_count / (word_count + 1e-10), \n",
    "                                 unique_upper_word_count, unique_upper_word_count / (upper_word_count + 1e-10))\n",
    "\n",
    "        return dataset.withColumn(self.getOutputCol(), \n",
    "                                  F.udf(f, VectorUDT())(dataset[self.getInputCol()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 1 µs, total: 8 µs\n",
      "Wall time: 13.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# show model prediction performance on the given dataset\n",
    "def eval_model_perf(fitted_model, dataset, label_col=\"label\", prediction_col=\"prediction\", probability_col=\"probability\"):\n",
    "    pred_dataset = fitted_model.transform(dataset)\n",
    "    eval_dataset = pred_dataset.select(label_col, prediction_col, probability_col)\n",
    "    # model performance evaluation\n",
    "    metricNames = [\"accuracy\", \"f1\"]\n",
    "    model_eval = MulticlassClassificationEvaluator(predictionCol=prediction_col, labelCol=label_col)\n",
    "    for m in metricNames:\n",
    "        val = model_eval.evaluate(eval_dataset, {model_eval.metricName: m})\n",
    "        print(m, \" = \", val)\n",
    "    roc_eval = BinaryClassificationEvaluator(rawPredictionCol=probability_col, labelCol=label_col, metricName=\"areaUnderROC\")\n",
    "    print(\"AUC =\", roc_eval.evaluate(eval_dataset))    \n",
    "    return pred_dataset\n",
    "\n",
    "# show CV param tunning result\n",
    "def show_cv_results(cv_model):\n",
    "    for result, param in sorted(zip(cv_model.avgMetrics, cv_model.getEstimatorParamMaps()), reverse=True, key=lambda x: x[0]):\n",
    "        print(result, \" | \", param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def build_data_preproc_model_with_pca(vocab_size=5000):\n",
    "    preproc_steps = [\n",
    "        RegexTokenizer(inputCol=\"content\", outputCol=\"all_words\", pattern=r\"\\W\"),\n",
    "        StopWordsRemover(inputCol=\"all_words\", outputCol=\"words\"),\n",
    "        CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=vocab_size),\n",
    "        IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\"),\n",
    "        PCA(inputCol=\"tfidf_features\", outputCol=\"pca_features\", k=100),\n",
    "        ReviewContentTransformer(inputCol=\"content\", outputCol=\"content_features\"),\n",
    "        ReviewWordsTransformer(inputCol=\"words\", outputCol=\"word_features\"),\n",
    "        VectorAssembler(inputCols=[\"pca_features\", \"content_features\", \"word_features\"], \n",
    "                        outputCol=\"features\")\n",
    "    ]\n",
    "    return Pipeline(stages=preproc_steps)\n",
    "\n",
    "def build_data_preproc_model_without_pca(vocab_size=5000):\n",
    "    preproc_steps = [\n",
    "        RegexTokenizer(inputCol=\"content\", outputCol=\"all_words\", pattern=r\"\\W\"),\n",
    "        StopWordsRemover(inputCol=\"all_words\", outputCol=\"words\"),\n",
    "        CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=vocab_size),\n",
    "        IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\"),\n",
    "        ReviewContentTransformer(inputCol=\"content\", outputCol=\"content_features\"),\n",
    "        ReviewWordsTransformer(inputCol=\"words\", outputCol=\"word_features\"),\n",
    "        VectorAssembler(inputCols=[\"tf_features\", \"content_features\", \"word_features\"], outputCol=\"features\")\n",
    "    ]\n",
    "    return Pipeline(stages=preproc_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Run Models with PCA Features**********\n",
      "CPU times: user 235 ms, sys: 41 ms, total: 276 ms\n",
      "Wall time: 3min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"**********Run Models with PCA Features**********\")\n",
    "# generate the features to be used for model training\n",
    "preproc_model = build_data_preproc_model_with_pca(3000).fit(df_train)\n",
    "df_train_pca = preproc_model.transform(df_train).select(\"label\", \"features\")\n",
    "df_test_pca = preproc_model.transform(df_test).select(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.8887974770327711\n",
      "f1  =  0.8886981347903793\n",
      "AUC = 0.9493754470855285\n",
      "CPU times: user 287 ms, sys: 81.5 ms, total: 369 ms\n",
      "Wall time: 13min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr_model = LogisticRegression(featuresCol='features', \n",
    "                              labelCol='label', \n",
    "                              predictionCol='prediction', \n",
    "                              probabilityCol='probability', \n",
    "                              rawPredictionCol='rawPrediction',\n",
    "                              family='binomial', \n",
    "                              fitIntercept=True, \n",
    "                              threshold=0.5, \n",
    "                              standardization=False, \n",
    "                              maxIter=200, \n",
    "                              regParam=0.005, \n",
    "                              elasticNetParam=0, \n",
    "                              tol=1e-06, \n",
    "                              aggregationDepth=2)\n",
    "\n",
    "lr_model = lr_model.fit(df_train_pca)\n",
    "eval_model_perf(lr_model, df_test_pca)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.8318250377073907\n",
      "f1  =  0.8317973776701691\n",
      "AUC = 0.9019345480414802\n",
      "CPU times: user 102 ms, sys: 25.5 ms, total: 127 ms\n",
      "Wall time: 5min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dt_model = DecisionTreeClassifier(featuresCol='features', \n",
    "                                  labelCol='label', \n",
    "                                  predictionCol='prediction', \n",
    "                                  probabilityCol='probability', \n",
    "                                  rawPredictionCol='rawPrediction', \n",
    "                                  maxDepth=10, maxBins=32, \n",
    "                                  minInstancesPerNode=1, \n",
    "                                  minInfoGain=0.0, \n",
    "                                  maxMemoryInMB=2048, \n",
    "                                  cacheNodeIds=True, \n",
    "                                  checkpointInterval=10,\n",
    "                                  impurity='gini', \n",
    "                                  seed=666)\n",
    "\n",
    "dt_model = dt_model.fit(df_train_pca)\n",
    "eval_model_perf(dt_model, df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.8716120480826364\n",
      "f1  =  0.8715955519837728\n",
      "AUC = 0.9467938127176144\n",
      "CPU times: user 134 ms, sys: 16.5 ms, total: 151 ms\n",
      "Wall time: 7min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf_model = RandomForestClassifier(featuresCol='features', \n",
    "                                  labelCol='label', \n",
    "                                  predictionCol='prediction', \n",
    "                                  probabilityCol='probability', \n",
    "                                  rawPredictionCol='rawPrediction',\n",
    "                                  maxDepth=10, \n",
    "                                  maxBins=32, \n",
    "                                  minInstancesPerNode=1, \n",
    "                                  minInfoGain=0.0, \n",
    "                                  maxMemoryInMB=2048, \n",
    "                                  cacheNodeIds=True, \n",
    "                                  checkpointInterval=10, \n",
    "                                  impurity='gini', \n",
    "                                  numTrees=200, \n",
    "                                  featureSubsetStrategy='auto', \n",
    "                                  seed=666, \n",
    "                                  subsamplingRate=0.8)\n",
    "\n",
    "rf_model = rf_model.fit(df_train_pca)\n",
    "eval_model_perf(rf_model, df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.9042689336806984\n",
      "f1  =  0.904267989805318\n",
      "AUC = 0.9677402451839049\n",
      "CPU times: user 1.87 s, sys: 554 ms, total: 2.42 s\n",
      "Wall time: 1h 36min 21s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gbt_model = GBTClassifier(featuresCol='features', labelCol='label', maxIter=250)\n",
    "\n",
    "gbt_model = gbt_model.fit(df_train_pca)\n",
    "eval_model_perf(gbt_model, df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Run Models without PCA Features**********\n",
      "CPU times: user 195 ms, sys: 35.1 ms, total: 230 ms\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"**********Run Models without PCA Features**********\")\n",
    "# generate the features to be used for model training\n",
    "preproc_model = build_data_preproc_model_without_pca(3000).fit(df_train)\n",
    "df_train_wo_pca = preproc_model.transform(df_train).select(\"label\", \"features\")\n",
    "df_test_wo_pca = preproc_model.transform(df_test).select(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.9460670048905343\n",
      "f1  =  0.9460646583293852\n",
      "AUC = 0.9850949220068903\n",
      "CPU times: user 274 ms, sys: 85.3 ms, total: 360 ms\n",
      "Wall time: 13min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr_model = LogisticRegression(featuresCol='features', \n",
    "                              labelCol='label', \n",
    "                              predictionCol='prediction', \n",
    "                              probabilityCol='probability', \n",
    "                              rawPredictionCol='rawPrediction',\n",
    "                              family='binomial', \n",
    "                              fitIntercept=True, \n",
    "                              threshold=0.5, \n",
    "                              standardization=False, \n",
    "                              maxIter=200, \n",
    "                              regParam=0.005, \n",
    "                              elasticNetParam=0, \n",
    "                              tol=1e-06, \n",
    "                              aggregationDepth=2)\n",
    "\n",
    "lr_model = lr_model.fit(df_train_wo_pca)    \n",
    "eval_model_perf(lr_model, df_test_wo_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.9298414004296357\n",
      "f1  =  0.9298277334371616\n",
      "AUC = 0.971089280360815\n",
      "CPU times: user 90.3 ms, sys: 34.7 ms, total: 125 ms\n",
      "Wall time: 5min 29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dt_model = DecisionTreeClassifier(featuresCol='features', \n",
    "                                  labelCol='label', \n",
    "                                  predictionCol='prediction', \n",
    "                                  probabilityCol='probability', \n",
    "                                  rawPredictionCol='rawPrediction', \n",
    "                                  maxDepth=10, maxBins=32, \n",
    "                                  minInstancesPerNode=1, \n",
    "                                  minInfoGain=0.0, \n",
    "                                  maxMemoryInMB=2048, \n",
    "                                  cacheNodeIds=True, \n",
    "                                  checkpointInterval=10,\n",
    "                                  impurity='gini', \n",
    "                                  seed=666)\n",
    "\n",
    "dt_model = dt_model.fit(df_train_wo_pca)\n",
    "eval_model_perf(dt_model, df_test_wo_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.9332921979980804\n",
      "f1  =  0.93327484277938\n",
      "AUC = 0.9852645370831858\n",
      "CPU times: user 93.8 ms, sys: 50.9 ms, total: 145 ms\n",
      "Wall time: 6min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf_model = RandomForestClassifier(featuresCol='features', \n",
    "                                  labelCol='label', \n",
    "                                  predictionCol='prediction', \n",
    "                                  probabilityCol='probability', \n",
    "                                  rawPredictionCol='rawPrediction',\n",
    "                                  maxDepth=10, \n",
    "                                  maxBins=32, \n",
    "                                  minInstancesPerNode=1, \n",
    "                                  minInfoGain=0.0, \n",
    "                                  maxMemoryInMB=2048, \n",
    "                                  cacheNodeIds=True, \n",
    "                                  checkpointInterval=10, \n",
    "                                  impurity='gini', \n",
    "                                  numTrees=200, \n",
    "                                  featureSubsetStrategy='auto', \n",
    "                                  seed=666, \n",
    "                                  subsamplingRate=0.8)\n",
    "\n",
    "rf_model = rf_model.fit(df_train_wo_pca)\n",
    "eval_model_perf(rf_model, df_test_wo_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.9588080831145108\n",
      "f1  =  0.9588033405381915\n",
      "AUC = 0.994218554245741\n",
      "CPU times: user 2.21 s, sys: 751 ms, total: 2.96 s\n",
      "Wall time: 3h 19min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gbt_model = GBTClassifier(featuresCol='features', labelCol='label', maxIter=250)\n",
    "\n",
    "gbt_model = gbt_model.fit(df_train_wo_pca)\n",
    "eval_model_perf(gbt_model, df_test_wo_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.8726338623130494\n",
      "f1  =  0.8726044296899041\n",
      "AUC = 0.9147177772916767\n",
      "CPU times: user 89 ms, sys: 16.7 ms, total: 106 ms\n",
      "Wall time: 5min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nb_model = NaiveBayes(featuresCol='features', \n",
    "                              labelCol='label', \n",
    "                              predictionCol='prediction', \n",
    "                              probabilityCol='probability', \n",
    "                              rawPredictionCol='rawPrediction', \n",
    "                              smoothing=1, \n",
    "                              modelType='multinomial')\n",
    "\n",
    "nb_model = nb_model.fit(df_train_wo_pca)\n",
    "eval_model_perf(nb_model, df_test_wo_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_train_sample = df_news_train.sample(5000)\n",
    "df_news_train_sample.to_csv(\"news_data_sample_small.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload from local news_data_sample_small.csv to data/fake-news/news_data_sample_small.csv\n"
     ]
    }
   ],
   "source": [
    "bucket_root_path = \"weicheng30417\"\n",
    "project_data_folder = \"data/fake-news/\"\n",
    "\n",
    "def upload_files(files):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_root_path)\n",
    "    for file in files:\n",
    "        butcketFile = project_data_folder + file\n",
    "        blob = bucket.blob(butcketFile)\n",
    "        blob.upload_from_filename(file)\n",
    "        print(\"Upload from local {0} to {1}\".format(file, butcketFile))\n",
    "\n",
    "\n",
    "upload_files([\"news_data_sample_small.csv\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

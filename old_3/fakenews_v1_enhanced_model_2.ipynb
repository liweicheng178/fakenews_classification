{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "3.6.10\nCPU times: user 62.7 ms, sys: 12.3 ms, total: 75 ms\nWall time: 73.4 ms\n"}], "source": "%%time\nimport pandas as pd\nimport os, sys, time, json, re, string, datetime\n\nfrom pyspark import SparkContext, SparkConf, StorageLevel, keyword_only\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.ml.linalg import Vectors, VectorUDT\nfrom pyspark.ml.param.shared import HasInputCol, HasInputCols, HasOutputCol, HasOutputCols, Param\nfrom pyspark.ml.feature import OneHotEncoder, HashingTF, IDF, Tokenizer, RegexTokenizer, NGram, CountVectorizer\nfrom pyspark.ml.feature import StopWordsRemover, VectorAssembler, PCA, OneHotEncoderEstimator,StringIndexer\n\nfrom pyspark.ml.classification import LogisticRegression, NaiveBayes, DecisionTreeClassifier, RandomForestClassifier\nfrom pyspark.ml.classification import GBTClassifier, MultilayerPerceptronClassifier\n\nfrom pyspark.ml import Pipeline, Transformer\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom platform import python_version\nprint(python_version())"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 4 \u00b5s, sys: 1e+03 ns, total: 5 \u00b5s\nWall time: 6.91 \u00b5s\n"}, {"data": {"text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://cluster-8f64-m.asia-southeast1-a.c.weicheng.internal:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.5</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        ", "text/plain": "<SparkContext master=yarn appName=PySparkShell>"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "%%time\nsc"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 6.55 ms, sys: 0 ns, total: 6.55 ms\nWall time: 9.81 ms\n"}], "source": "%%time\nspark = SparkSession.builder \\\n        .appName(\"fakenews\") \\\n        .config(\"spark.master\", \"yarn\") \\\n        .config(\"spark.submit.deployMode\", \"cluster\") \\\n        .config(\"spark.driver.memory\", \"25g\") \\\n        .config(\"spark.executor.instances\", \"5\") \\\n        .config(\"spark.executor.cores\", \"4\") \\\n        .config(\"spark.executor.memory\", \"25g\") \\\n        .getOrCreate()"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 12.6 ms, sys: 3.71 ms, total: 16.3 ms\nWall time: 6.32 s\n"}], "source": "%%time\nfakenews_path=\"gs://dataproc-6ca41800-27b4-47d5-abee-55c011dfa389-asia-southeast1/data/fake-news/\"\nfakenews_data_path = fakenews_path + \"two_million_rows_news_cleaned_2018_02_13_pyspark.csv\"\ndf_news = spark.read.format(\"com.databricks.spark.csv\") \\\n                    .option(\"header\", \"true\") \\\n                    .option(\"delimiter\", '#') \\\n                    .load(fakenews_data_path)\n\n#remove empty content which will cause problem when transform the text\ndf_news = df_news.filter(df_news.type != \"\")\ndf_news = df_news.filter(df_news.content != \"\")\ndf_news = df_news.filter(df_news.domain != \"\")\ndf_news = df_news.filter(df_news.title != \"\")\ndf_news = df_news.filter(df_news.authors != \"\")\ndf_news = df_news.dropDuplicates(['type', 'content', 'title', 'authors'])"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "# only keep type and content\ndf_news = df_news.select('type', 'content', 'domain', 'title','authors')\nsplit_authors_col = F.split(df_news['authors'], ', ')\ndf_news = df_news.withColumn('author1', split_authors_col.getItem(0))\ndf_news = df_news.withColumn('author2', split_authors_col.getItem(1))\n\n# add binary label\ndf_news = df_news.withColumn(\"label\", F.when(F.col(\"type\") == 'fake', 1).otherwise(0))\n\ndf_news_fake = df_news.filter(df_news.type == 'fake')\ndf_news_nonfake = df_news.filter(df_news.type != 'fake')\ndf_news = df_news_fake.union(df_news_nonfake)\n\n# split the dataset\ndf_train, df_test = df_news.randomSplit([0.8, 0.2], seed=666)\nparam_tuning = False"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 99 \u00b5s, sys: 10 \u00b5s, total: 109 \u00b5s\nWall time: 115 \u00b5s\n"}], "source": "%%time\n# customized transformer class to manually extract some counting based text features\nclass ReviewContentTransformer(Transformer, HasInputCol, HasOutputCol):\n\n    @keyword_only\n    def __init__(self, inputCol=\"content\", outputCol=\"content_features\"):\n        super(ReviewContentTransformer, self).__init__()\n        kwargs = self._input_kwargs\n        self.setParams(**kwargs)\n\n    @keyword_only\n    def setParams(self, inputCol=None, outputCol=None):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n\n    def _transform(self, dataset):\n        \n        def f(s):\n            uppercase_count = 0\n            char_count = 0\n            for c in s:                \n                if c in string.ascii_uppercase:\n                    uppercase_count += 1\n                    char_count += 1\n                elif c in string.ascii_lowercase:\n                    char_count += 1\n            \n            text_len = len(s)\n            return Vectors.dense(text_len, char_count, \n                                 uppercase_count, uppercase_count / (char_count + 1e-10))\n\n        return dataset.withColumn(self.getOutputCol(), \n                                  F.udf(f, VectorUDT())(dataset[self.getInputCol()]))"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 114 \u00b5s, sys: 0 ns, total: 114 \u00b5s\nWall time: 119 \u00b5s\n"}], "source": "%%time\n# customized transformer class to manually extract some counting based word features\nclass ReviewWordsTransformer(Transformer, HasInputCol, HasOutputCol):\n\n    @keyword_only\n    def __init__(self, inputCol=\"content\", outputCol=\"content_features\"):\n        super(ReviewWordsTransformer, self).__init__()\n        kwargs = self._input_kwargs\n        self.setParams(**kwargs)\n\n    @keyword_only\n    def setParams(self, inputCol=None, outputCol=None):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n\n    def _transform(self, dataset):\n        \n        def f(words):    \n            word_count = len(words)\n            unique_word_count = len(set(words))\n            upper_words = []\n            for w in words:\n                if w.isupper():\n                    upper_words.append(w)\n            upper_word_count = len(set(upper_words))\n            unique_upper_word_count = len(upper_words)\n            return Vectors.dense(word_count, unique_word_count, unique_word_count / (word_count + 1e-10),\n                                 upper_word_count, upper_word_count / (word_count + 1e-10), \n                                 unique_upper_word_count, unique_upper_word_count / (upper_word_count + 1e-10))\n\n        return dataset.withColumn(self.getOutputCol(), \n                                  F.udf(f, VectorUDT())(dataset[self.getInputCol()]))"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 7 \u00b5s, sys: 1 \u00b5s, total: 8 \u00b5s\nWall time: 11.9 \u00b5s\n"}], "source": "%%time\n# show model prediction performance on the given dataset\ndef eval_model_perf(fitted_model, dataset, label_col=\"label\", prediction_col=\"prediction\", probability_col=\"probability\"):\n    pred_dataset = fitted_model.transform(dataset)\n    eval_dataset = pred_dataset.select(label_col, prediction_col, probability_col)\n    # model performance evaluation\n    metricNames = [\"accuracy\", \"f1\"]\n    model_eval = MulticlassClassificationEvaluator(predictionCol=prediction_col, labelCol=label_col)\n    for m in metricNames:\n        val = model_eval.evaluate(eval_dataset, {model_eval.metricName: m})\n        print(m, \" = \", val)\n    roc_eval = BinaryClassificationEvaluator(rawPredictionCol=probability_col, labelCol=label_col, metricName=\"areaUnderROC\")\n    print(\"AUC =\", roc_eval.evaluate(eval_dataset))    \n    return pred_dataset\n\n# show CV param tunning result\ndef show_cv_results(cv_model):\n    for result, param in sorted(zip(cv_model.avgMetrics, cv_model.getEstimatorParamMaps()), reverse=True, key=lambda x: x[0]):\n        print(result, \" | \", param)"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "CPU times: user 7 \u00b5s, sys: 0 ns, total: 7 \u00b5s\nWall time: 12.2 \u00b5s\n"}], "source": "%%time\ndef build_data_preproc_model_with_pca(vocab_size=5000):\n    preproc_steps = [\n        RegexTokenizer(inputCol=\"content\", outputCol=\"all_words\", pattern=r\"\\W\"),\n        StopWordsRemover(inputCol=\"all_words\", outputCol=\"words\"),\n        CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=vocab_size),\n        IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\"),\n        PCA(inputCol=\"tfidf_features\", outputCol=\"pca_features\", k=100),\n        \n        ReviewContentTransformer(inputCol=\"content\", outputCol=\"content_features\"),\n        ReviewWordsTransformer(inputCol=\"words\", outputCol=\"word_features\"),\n        \n        RegexTokenizer(inputCol=\"title\", outputCol=\"all_title_words\", pattern=r\"\\W\"),\n        StopWordsRemover(inputCol=\"all_title_words\", outputCol=\"title_words\"),\n        CountVectorizer(inputCol=\"title_words\", outputCol=\"title_tf_features\", vocabSize=100),\n        IDF(inputCol=\"title_tf_features\", outputCol=\"title_tfidf_features\"),\n        PCA(inputCol=\"title_tfidf_features\", outputCol=\"title_pca_features\", k=100),        \n\n        StringIndexer(inputCol=\"domain\", outputCol=\"domain_indexed\", handleInvalid='keep'),\n        OneHotEncoder(inputCol=\"domain_indexed\", outputCol=\"domain_feature\"),\n        \n        StringIndexer(inputCol=\"author1\", outputCol=\"author1_indexed\", handleInvalid='keep'),\n        OneHotEncoder(inputCol=\"author1_indexed\", outputCol=\"author1_feature\"),\n        \n        StringIndexer(inputCol=\"author2\", outputCol=\"author2_indexed\", handleInvalid='keep'),        \n        OneHotEncoder(inputCol=\"author2_indexed\", outputCol=\"author2_feature\"),\n        \n        VectorAssembler(inputCols=[\"pca_features\", \"title_pca_features\", \"title_tfidf_features\", \n                                   \"content_features\", \"word_features\", \"domain_feature\",\"author1_feature\", \"author2_feature\"], \n                        outputCol=\"features\")\n    ]\n    return Pipeline(stages=preproc_steps)\n\ndef build_data_preproc_model_without_pca(vocab_size=5000):\n    preproc_steps = [\n        RegexTokenizer(inputCol=\"content\", outputCol=\"all_words\", pattern=r\"\\W\"),\n        StopWordsRemover(inputCol=\"all_words\", outputCol=\"words\"),\n        \n        StringIndexer(inputCol=\"domain\", outputCol=\"domain_indexed\", handleInvalid='keep'),\n        OneHotEncoder(inputCol=\"domain_indexed\", outputCol=\"domain_feature\"),\n        \n        StringIndexer(inputCol=\"author1\", outputCol=\"author1_indexed\", handleInvalid='keep'),\n        OneHotEncoder(inputCol=\"author1_indexed\", outputCol=\"author1_feature\"),\n        \n        StringIndexer(inputCol=\"author2\", outputCol=\"author2_indexed\", handleInvalid='keep'),        \n        OneHotEncoder(inputCol=\"author2_indexed\", outputCol=\"author2_feature\"),\n        \n        ReviewContentTransformer(inputCol=\"content\", outputCol=\"content_features\"),\n        ReviewWordsTransformer(inputCol=\"words\", outputCol=\"word_features\"),\n        \n        VectorAssembler(inputCols=[\"tf_features\", \"content_features\", \"word_features\", \n                                   \"domain_feature\",\"author1_feature\", \"author2_feature\"], \n                        outputCol=\"features\")\n    ]\n    return Pipeline(stages=preproc_steps)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "**********Run Models with PCA Features**********\n[Row(label=1, features=SparseVector(4920, {0: -1.545, 1: -10.1689, 2: -4.6902, 3: 1.731, 4: -3.6121, 5: 2.793, 6: -2.1007, 7: 0.9937, 8: -1.9308, 9: 2.6694, 10: 0.6508, 11: 0.7024, 12: 0.5505, 13: -3.1285, 14: -2.071, 15: 0.6434, 16: 1.8526, 17: 1.1357, 18: -2.4302, 19: -1.8551, 20: -0.1209, 21: 0.1592, 22: 0.391, 23: -1.0441, 24: -0.0244, 25: -2.1715, 26: -4.2558, 27: 0.0521, 28: 0.0537, 29: 0.9504, 30: 0.2269, 31: -1.1771, 32: -0.7934, 33: 0.4121, 34: -0.5933, 35: 2.1907, 36: -0.419, 37: -0.187, 38: -0.499, 39: -0.8866, 40: -0.7528, 41: -1.1466, 42: 0.9674, 43: 0.8339, 44: 0.4442, 45: -0.7517, 46: -0.006, 47: -1.0215, 48: -0.339, 49: 0.5213, 50: -1.6145, 51: -1.5744, 52: -0.9032, 53: -0.8733, 54: -0.396, 55: -0.4403, 56: -0.7994, 57: 0.5004, 58: -2.1218, 59: 0.5531, 60: 1.3719, 61: 2.4516, 62: -0.4075, 63: 1.6629, 64: 0.2582, 65: 0.0971, 66: 1.035, 67: -0.5552, 68: 1.4545, 69: 1.6198, 70: 1.99, 71: 0.3013, 72: 0.8973, 73: 0.6923, 74: -0.7673, 75: 0.6783, 76: -0.5508, 77: -1.3549, 78: -1.1685, 79: -0.526, 80: -1.9426, 81: -0.5479, 82: 1.3007, 83: 2.0831, 84: 2.3166, 85: -1.4189, 86: -1.2969, 87: -0.3217, 88: -1.3419, 89: -0.2359, 90: 1.2087, 91: 3.365, 92: 0.302, 93: -2.0248, 94: 0.3691, 95: -0.7474, 96: -0.8639, 97: 0.7611, 98: -0.3284, 99: 0.9217, 100: 0.2702, 101: -0.1995, 102: -1.4313, 103: -0.0016, 104: 0.2921, 105: -2.7625, 106: -2.3423, 107: -0.1783, 108: -0.239, 109: 0.2451, 110: -0.1448, 111: 0.5533, 112: -0.4374, 113: -0.1764, 114: -0.1136, 115: -0.4464, 116: -0.9021, 117: 0.1621, 118: 0.9563, 119: 0.5787, 120: -0.0134, 121: 0.7975, 122: 1.5458, 123: 0.8521, 124: -1.6362, 125: -3.6928, 126: 0.8991, 127: -2.4713, 128: -2.1756, 129: -0.5226, 130: 0.7009, 131: -0.3312, 132: -1.4237, 133: 0.8565, 134: 1.0727, 135: 1.1581, 136: -0.9593, 137: -0.1673, 138: 0.5624, 139: -0.2002, 140: 0.0347, 141: -0.1065, 142: 0.2738, 143: 0.2687, 144: 0.4334, 145: 0.2616, 146: -0.5708, 147: -0.4345, 148: -0.7604, 149: 0.0756, 150: -0.6325, 151: -0.6275, 152: -0.1719, 153: -0.3564, 154: 0.2587, 155: -0.026, 156: -0.3012, 157: -0.1231, 158: 0.0759, 159: 0.2363, 160: -0.0444, 161: -0.1162, 162: -0.1015, 163: 0.068, 164: -0.0326, 165: 0.0031, 166: -0.0842, 167: 0.5742, 168: -0.1711, 169: 0.0684, 170: -0.1629, 171: 0.1204, 172: 0.2755, 173: 0.0895, 174: 0.0828, 175: -0.2487, 176: 0.0109, 177: 0.0219, 178: -0.091, 179: -0.3986, 180: 0.048, 181: 0.0247, 182: -0.0014, 183: 0.1119, 184: 0.131, 185: 0.0128, 186: 0.4679, 187: 0.0277, 188: -0.1029, 189: -0.0166, 190: 0.0967, 191: -0.0664, 192: -0.2106, 193: 0.0193, 194: -0.0645, 195: 0.2744, 196: 0.0532, 197: -0.0301, 198: 0.103, 199: 0.0094, 206: 3.9484, 213: 4.5847, 277: 4.978, 300: 3213.0, 301: 2548.0, 302: 73.0, 303: 0.0286, 304: 304.0, 305: 243.0, 306: 0.7993, 411: 1.0, 3304: 1.0}))]\n[Row(label=1, features=SparseVector(4920, {0: -7.7425, 1: -22.536, 2: -9.8002, 3: 3.7714, 4: -12.4032, 5: 1.4391, 6: 5.4719, 7: 5.645, 8: -3.8413, 9: 0.9303, 10: 1.5387, 11: -21.4869, 12: 10.5836, 13: 4.654, 14: 2.1409, 15: -0.6979, 16: 0.8552, 17: -2.6763, 18: 2.2158, 19: -2.0051, 20: -3.7364, 21: -1.1563, 22: -0.0596, 23: 0.9812, 24: -1.7337, 25: -0.8933, 26: -0.2296, 27: -4.9756, 28: 0.9534, 29: -1.5056, 30: 3.6268, 31: 2.2676, 32: -3.307, 33: -0.3575, 34: 1.0813, 35: -0.1367, 36: 1.5719, 37: -0.8179, 38: 2.165, 39: -5.7475, 40: 1.088, 41: -1.6793, 42: -1.0604, 43: 3.3757, 44: 1.1844, 45: 3.4564, 46: 1.2375, 47: 2.0531, 48: 3.2528, 49: -4.6676, 50: 5.8678, 51: -1.438, 52: 2.814, 53: 0.4633, 54: 1.1012, 55: 2.6736, 56: 1.5464, 57: 1.0978, 58: -6.708, 59: -1.5252, 60: 1.5264, 61: -3.8512, 62: 0.1683, 63: 2.407, 64: -6.2174, 65: -1.1937, 66: 0.3069, 67: 1.8625, 68: -2.7966, 69: 5.7119, 70: 5.3144, 71: -0.1153, 72: 9.2927, 73: 1.5412, 74: -10.288, 75: -1.6128, 76: 7.0432, 77: -7.6114, 78: 5.558, 79: -2.8402, 80: -2.7393, 81: 6.5634, 82: -2.4276, 83: 2.5808, 84: -3.689, 85: -4.1073, 86: 1.1082, 87: -2.4283, 88: -4.8587, 89: 6.2358, 90: -5.6357, 91: -6.0322, 92: -9.5435, 93: -1.5215, 94: -0.9911, 95: -3.8434, 96: -0.6488, 97: 1.3273, 98: 5.9696, 99: -2.2133, 100: 0.0745, 101: -0.0663, 102: 0.0219, 103: 0.0888, 104: 0.0402, 105: 0.1542, 106: -0.0105, 107: 0.0835, 108: -0.0121, 109: -0.3036, 110: 0.0128, 111: 0.1112, 112: -0.171, 113: -0.2051, 114: 0.1917, 115: -0.0012, 116: -0.2558, 117: -0.7805, 118: 1.0644, 119: -1.0609, 120: 2.2647, 121: -1.3045, 122: 0.958, 123: -2.2195, 124: -0.0453, 125: 0.6214, 126: 1.0989, 127: -0.2374, 128: -0.3921, 129: -0.2096, 130: -0.9558, 131: 0.541, 132: -0.482, 133: 0.3438, 134: 0.8905, 135: 0.2286, 136: 0.2481, 137: -0.143, 138: -0.0298, 139: 0.715, 140: 0.3673, 141: -0.2135, 142: 0.0885, 143: 0.2105, 144: -0.1257, 145: 0.0515, 146: 0.0839, 147: 0.1043, 148: 0.0797, 149: 0.0321, 150: 0.0893, 151: -0.0913, 152: -0.1608, 153: 0.0526, 154: -0.239, 155: -0.0303, 156: -0.039, 157: 0.06, 158: 0.0204, 159: 0.1809, 160: -0.0192, 161: -0.1163, 162: 0.0134, 163: -0.3204, 164: -0.19, 165: -0.1737, 166: -0.1642, 167: 0.126, 168: 0.0866, 169: 0.147, 170: 0.0138, 171: 0.2662, 172: 0.0454, 173: -0.1183, 174: 0.0106, 175: 0.0765, 176: 0.0211, 177: -0.0642, 178: -0.1608, 179: 0.1571, 180: -0.0308, 181: -0.1178, 182: -0.0159, 183: 0.0857, 184: 0.1813, 185: 0.0031, 186: 0.0167, 187: 0.0007, 188: -0.113, 189: -0.0757, 190: 0.0342, 191: 0.1711, 192: 0.0657, 193: -0.1001, 194: 0.0502, 195: 0.0675, 196: -0.0402, 197: 0.0005, 198: 0.0033, 199: -0.0068, 224: 4.6546, 300: 8161.0, 301: 6389.0, 302: 223.0, 303: 0.0349, 304: 799.0, 305: 458.0, 306: 0.5732, 411: 1.0}))]\n**********LogisticRegression**********\naccuracy  =  0.9925576779955346\nf1  =  0.9944252472513173\nAUC = 0.9986527956337812\ntime taken for LogisticRegression:  0:01:41.208332\n**********DecisionTreeClassifier**********\naccuracy  =  1.0\nf1  =  0.9997519212251108\nAUC = 0.9994926433282598\ntime taken for DecisionTreeClassifier:  0:02:01.595194\n**********RandomForestClassifier**********\naccuracy  =  0.9560903001736542\nf1  =  0.958994175214376\nAUC = 0.9956756463871824\ntime taken for RandomForestClassifier:  0:04:15.905394\n**********GBTClassifier**********\naccuracy  =  1.0\nf1  =  0.9997517966722368\nAUC = 1.0\ntime taken for GBTClassifier:  0:21:26.844512\n**********MultilayerPerceptronClassifier**********\naccuracy  =  0.9610518481766311\nf1  =  0.9601511092415873\nAUC = 0.9952101589553378\ntime taken for MultilayerPerceptronClassifier:  0:02:21.064868\nCPU times: user 1.72 s, sys: 535 ms, total: 2.25 s\nWall time: 35min 24s\n"}], "source": "%%time\nprint(\"**********Run Models with PCA Features**********\")\n# generate the features to be used for model training\npreproc_model = build_data_preproc_model_with_pca(2000).fit(df_train)\ndf_train_pca = preproc_model.transform(df_train).select(\"label\", \"features\")\ndf_test_pca = preproc_model.transform(df_test).select(\"label\", \"features\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "%%time\nlr_model = LogisticRegression(featuresCol='features', \n                              labelCol='label', \n                              predictionCol='prediction', \n                              probabilityCol='probability', \n                              rawPredictionCol='rawPrediction',\n                              family='binomial', \n                              fitIntercept=True, \n                              threshold=0.5, \n                              standardization=False, \n                              maxIter=200, \n                              regParam=0.005, \n                              elasticNetParam=0, \n                              tol=1e-06, \n                              aggregationDepth=2)\n\nlr_model = lr_model.fit(df_train_pca)\neval_model_perf(lr_model, df_test_pca)  "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "%%time\ndt_model = DecisionTreeClassifier(featuresCol='features', \n                                  labelCol='label', \n                                  predictionCol='prediction', \n                                  probabilityCol='probability', \n                                  rawPredictionCol='rawPrediction', \n                                  maxDepth=10, maxBins=32, \n                                  minInstancesPerNode=1, \n                                  minInfoGain=0.0, \n                                  maxMemoryInMB=2048, \n                                  cacheNodeIds=True, \n                                  checkpointInterval=10,\n                                  impurity='gini', \n                                  seed=666)\n\ndt_model = dt_model.fit(df_train_pca)\neval_model_perf(dt_model, df_test_pca)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "%%time\nrf_model = RandomForestClassifier(featuresCol='features', \n                                  labelCol='label', \n                                  predictionCol='prediction', \n                                  probabilityCol='probability', \n                                  rawPredictionCol='rawPrediction',\n                                  maxDepth=10, \n                                  maxBins=32, \n                                  minInstancesPerNode=1, \n                                  minInfoGain=0.0, \n                                  maxMemoryInMB=2048, \n                                  cacheNodeIds=True, \n                                  checkpointInterval=10, \n                                  impurity='gini', \n                                  numTrees=200, \n                                  featureSubsetStrategy='auto', \n                                  seed=666, \n                                  subsamplingRate=0.8)\n\nrf_model = rf_model.fit(df_train_pca)\neval_model_perf(rf_model, df_test_pca)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "%%time\ngbt_model = GBTClassifier(featuresCol='features', \n                         labelCol='label', \n                         maxIter=250)\n\ngbt_model = gbt_model.fit(df_train_pca)\neval_model_perf(gbt_model, df_test_pca)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "%%time\nmp_model = MultilayerPerceptronClassifier(featuresCol='features', \n                                          labelCol='label', \n                                          predictionCol='prediction', \n                                          layers=[4, 5, 4, 3],  \n                                          maxIter=100, \n                                          blockSize=128, \n                                          seed=1234)    \n\n    \n\nmp_model = mp_model.fit(df_train_pca)\neval_model_perf(mp_model, df_test_pca)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "**********Run Models without PCA Features**********\n[Row(label=1, features=SparseVector(4645, {0: 4268.0, 1: 3398.0, 2: 126.0, 3: 0.0371, 4: 406.0, 5: 270.0, 6: 0.665, 94: 1.0}))]\n[Row(label=1, features=SparseVector(4645, {0: 16668.0, 1: 13355.0, 2: 443.0, 3: 0.0332, 4: 1545.0, 5: 895.0, 6: 0.5793, 94: 1.0}))]\naccuracy  =  0.9776730339866038\nf1  =  0.9816465297766493\nAUC = 0.9871456507711381\nCPU times: user 333 ms, sys: 91.1 ms, total: 424 ms\nWall time: 4min 14s\n"}, {"data": {"text/plain": "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "%%time\nprint(\"**********Run Models without PCA Features**********\")\n# generate the features to be used for model training\npreproc_model = build_data_preproc_model_without_pca(3000).fit(df_train)\ndf_train_wo_pca = preproc_model.transform(df_train).select(\"label\", \"features\")\ndf_test_wo_pca = preproc_model.transform(df_test).select(\"label\", \"features\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "%%time\nlr_model = LogisticRegression(featuresCol='features', \n                              labelCol='label', \n                              predictionCol='prediction', \n                              probabilityCol='probability', \n                              rawPredictionCol='rawPrediction',\n                              family='binomial', \n                              fitIntercept=True, \n                              threshold=0.5, \n                              standardization=False, \n                              maxIter=200, \n                              regParam=0.005, \n                              elasticNetParam=0, \n                              tol=1e-06, \n                              aggregationDepth=2)\n\nlr_model = lr_model.fit(df_train_wo_pca)    \neval_model_perf(lr_model, df_test_wo_pca)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "%%time\ndt_model = DecisionTreeClassifier(featuresCol='features', \n                                  labelCol='label', \n                                  predictionCol='prediction', \n                                  probabilityCol='probability', \n                                  rawPredictionCol='rawPrediction', \n                                  maxDepth=10, maxBins=32, \n                                  minInstancesPerNode=1, \n                                  minInfoGain=0.0, \n                                  maxMemoryInMB=2048, \n                                  cacheNodeIds=True, \n                                  checkpointInterval=10,\n                                  impurity='gini', \n                                  seed=666)\n\ndt_model = dt_model.fit(df_train_wo_pca)\neval_model_perf(dt_model, df_test_wo_pca)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "%%time\nrf_model = RandomForestClassifier(featuresCol='features', \n                                  labelCol='label', \n                                  predictionCol='prediction', \n                                  probabilityCol='probability', \n                                  rawPredictionCol='rawPrediction',\n                                  maxDepth=10, \n                                  maxBins=32, \n                                  minInstancesPerNode=1, \n                                  minInfoGain=0.0, \n                                  maxMemoryInMB=2048, \n                                  cacheNodeIds=True, \n                                  checkpointInterval=10, \n                                  impurity='gini', \n                                  numTrees=200, \n                                  featureSubsetStrategy='auto', \n                                  seed=666, \n                                  subsamplingRate=0.8)\n\nrf_model = rf_model.fit(df_train_wo_pca)\neval_model_perf(rf_model, df_test_wo_pca)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "%%time\ngbt_model = GBTClassifier(featuresCol='features', \n                         labelCol='label', \n                         maxIter=250)\n\ngbt_model = gbt_model.fit(df_train_wo_pca)\neval_model_perf(gbt_model, df_test_wo_pca)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "%%time\nmp_model = MultilayerPerceptronClassifier(featuresCol='features', \n                                          labelCol='label', \n                                          predictionCol='prediction', \n                                          layers=[4, 5, 4, 3],  \n                                          maxIter=100, \n                                          blockSize=128, \n                                          seed=1234)\n\nmp_model = mp_model.fit(df_train_wo_pca)\neval_model_perf(mp_model, df_test_wo_pca)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "%%time\nnb_model = NaiveBayes(featuresCol='features', \n                              labelCol='label', \n                              predictionCol='prediction', \n                              probabilityCol='probability', \n                              rawPredictionCol='rawPrediction', \n                              smoothing=1, \n                              modelType='multinomial')\n\nnb_model = nb_model.fit(df_train_wo_pca)\neval_model_perf(nb_model, df_test_wo_pca)"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.10"}}, "nbformat": 4, "nbformat_minor": 2}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /opt/conda/anaconda/lib/python3.6/site-packages (20.0.2)\n",
      "Requirement already satisfied: gcsfs in /opt/conda/anaconda/lib/python3.6/site-packages (0.6.1)\n",
      "Requirement already satisfied: requests in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (2.23.0)\n",
      "Requirement already satisfied: google-auth>=1.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (1.11.3)\n",
      "Requirement already satisfied: decorator in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (4.4.2)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (0.4.1)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from gcsfs) (0.7.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests->gcsfs) (3.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (1.14.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (4.0.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (4.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs) (46.0.0.post20200309)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/anaconda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.10\n",
      "CPU times: user 52.1 ms, sys: 15.7 ms, total: 67.7 ms\n",
      "Wall time: 70.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import os, sys, time, json, re, string, datetime\n",
    "\n",
    "from pyspark import SparkContext, SparkConf, StorageLevel, keyword_only\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.param.shared import HasInputCol, HasInputCols, HasOutputCol, HasOutputCols, Param\n",
    "from pyspark.ml.feature import OneHotEncoder, HashingTF, IDF, Tokenizer, RegexTokenizer, NGram, CountVectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover, VectorAssembler, PCA, OneHotEncoderEstimator,StringIndexer\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier, MultilayerPerceptronClassifier\n",
    "\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-8544-m.asia-southeast1-a.c.weicheng-273112.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.95 ms, sys: 4.07 ms, total: 8.01 ms\n",
      "Wall time: 9.67 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"fakenews\") \\\n",
    "        .config(\"spark.master\", \"yarn\") \\\n",
    "        .config(\"spark.submit.deployMode\", \"cluster\") \\\n",
    "        .config(\"spark.driver.memory\", \"25g\") \\\n",
    "        .config(\"spark.executor.instances\", \"5\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.executor.memory\", \"25g\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 217806 entries, 0 to 108010\n",
      "Data columns (total 7 columns):\n",
      "domain             217806 non-null object\n",
      "type               217806 non-null object\n",
      "content            217806 non-null object\n",
      "title              217806 non-null object\n",
      "authors            217806 non-null object\n",
      "authors_missing    217806 non-null object\n",
      "label              217806 non-null object\n",
      "dtypes: object(7)\n",
      "memory usage: 13.3+ MB\n",
      "None\n",
      "CPU times: user 9.77 s, sys: 1.43 s, total: 11.2 s\n",
      "Wall time: 29.9 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>authors_missing</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>Boehner presses to make tax cuts permanent\\n\\n...</td>\n",
       "      <td>Boehner presses to make tax cuts permanent</td>\n",
       "      <td>United Liberty</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>An Armed Good Samaritan Who Doesn’t Want The S...</td>\n",
       "      <td>An Armed Good Samaritan Who Doesn’t Want The S...</td>\n",
       "      <td>The Real Revo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>(Before It's News)\\n\\nby Rob Morphy\\n\\nLegends...</td>\n",
       "      <td>Village Of The Dead: The Anjikuni Mystery</td>\n",
       "      <td>Rob Morphy, Mort Amsel</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>teaparty.org</td>\n",
       "      <td>fake</td>\n",
       "      <td>(Breitbart) – With his hysterical gotcha attac...</td>\n",
       "      <td>Trump Calls Out Race-Baiting ABC News Reporter</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>Researchers develop new depression diagnosis a...</td>\n",
       "      <td>Researchers develop new depression diagnosis a...</td>\n",
       "      <td>Bel Marra Health</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108006</th>\n",
       "      <td>sports.yahoo.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>View photos\\nMichigan guard Zak Irvin, right, ...</td>\n",
       "      <td>No. 25 Michigan beats Mount St. Mary's 64-47</td>\n",
       "      <td>The Associated Press</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108007</th>\n",
       "      <td>uk.finance.yahoo.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>President-elect Donald Trump continued his scr...</td>\n",
       "      <td>Trump quotes Hillary Clinton, rages against Wi...</td>\n",
       "      <td>Maxwell Tani</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108008</th>\n",
       "      <td>www.yahoo.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>Holiday shoppers eager to snag big discounts t...</td>\n",
       "      <td>Dialing up deals: Black Friday online sales hi...</td>\n",
       "      <td>ALEX VEIGA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108009</th>\n",
       "      <td>www.reuters.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>Kabul police raid shisha cafes in crackdown on...</td>\n",
       "      <td>Kabul police raid shisha cafes in crackdown on...</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108010</th>\n",
       "      <td>m.mlb.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>2016-17 MLB.com Hot Stove \\n© 2001- 2016 MLB A...</td>\n",
       "      <td>Possibility of Granderson in CF | MLB.com</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>217806 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      domain      type  \\\n",
       "0          beforeitsnews.com      fake   \n",
       "1          beforeitsnews.com      fake   \n",
       "2          beforeitsnews.com      fake   \n",
       "3               teaparty.org      fake   \n",
       "4          beforeitsnews.com      fake   \n",
       "...                      ...       ...   \n",
       "108006      sports.yahoo.com  reliable   \n",
       "108007  uk.finance.yahoo.com  reliable   \n",
       "108008         www.yahoo.com  reliable   \n",
       "108009       www.reuters.com  reliable   \n",
       "108010             m.mlb.com  reliable   \n",
       "\n",
       "                                                  content  \\\n",
       "0       Boehner presses to make tax cuts permanent\\n\\n...   \n",
       "1       An Armed Good Samaritan Who Doesn’t Want The S...   \n",
       "2       (Before It's News)\\n\\nby Rob Morphy\\n\\nLegends...   \n",
       "3       (Breitbart) – With his hysterical gotcha attac...   \n",
       "4       Researchers develop new depression diagnosis a...   \n",
       "...                                                   ...   \n",
       "108006  View photos\\nMichigan guard Zak Irvin, right, ...   \n",
       "108007  President-elect Donald Trump continued his scr...   \n",
       "108008  Holiday shoppers eager to snag big discounts t...   \n",
       "108009  Kabul police raid shisha cafes in crackdown on...   \n",
       "108010  2016-17 MLB.com Hot Stove \\n© 2001- 2016 MLB A...   \n",
       "\n",
       "                                                    title  \\\n",
       "0              Boehner presses to make tax cuts permanent   \n",
       "1       An Armed Good Samaritan Who Doesn’t Want The S...   \n",
       "2               Village Of The Dead: The Anjikuni Mystery   \n",
       "3          Trump Calls Out Race-Baiting ABC News Reporter   \n",
       "4       Researchers develop new depression diagnosis a...   \n",
       "...                                                   ...   \n",
       "108006       No. 25 Michigan beats Mount St. Mary's 64-47   \n",
       "108007  Trump quotes Hillary Clinton, rages against Wi...   \n",
       "108008  Dialing up deals: Black Friday online sales hi...   \n",
       "108009  Kabul police raid shisha cafes in crackdown on...   \n",
       "108010          Possibility of Granderson in CF | MLB.com   \n",
       "\n",
       "                       authors authors_missing label  \n",
       "0               United Liberty               0     1  \n",
       "1                The Real Revo               0     1  \n",
       "2       Rob Morphy, Mort Amsel               0     1  \n",
       "3                          nan               1     1  \n",
       "4             Bel Marra Health               0     1  \n",
       "...                        ...             ...   ...  \n",
       "108006    The Associated Press               0     0  \n",
       "108007            Maxwell Tani               0     0  \n",
       "108008              ALEX VEIGA               0     0  \n",
       "108009                     nan               1     0  \n",
       "108010                     nan               1     0  \n",
       "\n",
       "[217806 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_fake_train = pd.read_csv('gs://weicheng30417/data/fake-news/100k_fake_news_cleaned_dataset.csv')\n",
    "df_fake_train[['authors_missing']] = df_fake_train[['authors_missing']].astype(int)\n",
    "df_fake_train['label'] = 1\n",
    "df_reliable_train = pd.read_csv('gs://weicheng30417/data/fake-news/100k_reliable_news_cleaned_dataset.csv')\n",
    "df_reliable_train[['authors_missing']] = df_reliable_train[['authors_missing']].astype(int)\n",
    "df_reliable_train['label'] = 0\n",
    "df_news_train = pd.concat([df_fake_train, df_reliable_train])\n",
    "df_news_train[df_news_train.columns] = df_news_train[df_news_train.columns].astype(str)\n",
    "print(df_news_train.info())\n",
    "df_news_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+--------------------+--------------------+--------------------+---------------+-----+\n",
      "|           domain|type|             content|               title|             authors|authors_missing|label|\n",
      "+-----------------+----+--------------------+--------------------+--------------------+---------------+-----+\n",
      "|beforeitsnews.com|fake|Boehner presses t...|Boehner presses t...|      United Liberty|              0|    1|\n",
      "|beforeitsnews.com|fake|An Armed Good Sam...|An Armed Good Sam...|       The Real Revo|              0|    1|\n",
      "|beforeitsnews.com|fake|(Before It's News...|Village Of The De...|Rob Morphy, Mort ...|              0|    1|\n",
      "|     teaparty.org|fake|(Breitbart) – Wit...|Trump Calls Out R...|                 nan|              1|    1|\n",
      "|beforeitsnews.com|fake|Researchers devel...|Researchers devel...|    Bel Marra Health|              0|    1|\n",
      "|beforeitsnews.com|fake|Trading Watch Lis...|Trading Watch Lis...|Bulls On Wall Street|              0|    1|\n",
      "|beforeitsnews.com|fake|Looks like today ...|Looks like today ...|      Protein Wisdom|              0|    1|\n",
      "|beforeitsnews.com|fake|On the paradox of...|On the paradox of...|The Adam Smith In...|              0|    1|\n",
      "|beforeitsnews.com|fake|% of readers thin...|Minuteman- Most I...|         Nc Renegade|              0|    1|\n",
      "|beforeitsnews.com|fake|Juniper to Trim E...|Juniper to Trim E...|Zacks Investment ...|              0|    1|\n",
      "+-----------------+----+--------------------+--------------------+--------------------+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 13.6 s, sys: 892 ms, total: 14.5 s\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_news = spark.createDataFrame(df_news_train)\n",
    "df_news = df_news.withColumn(\"label\", df_news[\"label\"].cast(IntegerType()))\n",
    "df_news = df_news.withColumn(\"authors_missing\", df_news[\"authors_missing\"].cast(IntegerType()))\n",
    "df_news.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.67 ms, sys: 0 ns, total: 7.67 ms\n",
      "Wall time: 615 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#remove empty content which will cause problem when transform the text\n",
    "df_news = df_news.filter(df_news.content != \"\")\n",
    "df_news = df_news.filter(df_news.title != \"\")\n",
    "df_news = df_news.dropDuplicates(['label', 'content', 'title', 'authors_missing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep type and content\n",
    "df_news = df_news.select('label', 'content', 'title','authors_missing')\n",
    "\n",
    "# split the dataset\n",
    "df_train, df_test = df_news.randomSplit([0.8, 0.2], seed=666)\n",
    "param_tuning = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 113 µs, sys: 12 µs, total: 125 µs\n",
      "Wall time: 129 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# customized transformer class to manually extract some counting based text features\n",
    "class ReviewContentTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=\"content\", outputCol=\"content_features\"):\n",
    "        super(ReviewContentTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        def f(s):\n",
    "            uppercase_count = 0\n",
    "            char_count = 0\n",
    "            for c in s:                \n",
    "                if c in string.ascii_uppercase:\n",
    "                    uppercase_count += 1\n",
    "                    char_count += 1\n",
    "                elif c in string.ascii_lowercase:\n",
    "                    char_count += 1\n",
    "            \n",
    "            text_len = len(s)\n",
    "            return Vectors.dense(text_len, char_count, \n",
    "                                 uppercase_count, uppercase_count / (char_count + 1e-10))\n",
    "\n",
    "        return dataset.withColumn(self.getOutputCol(), \n",
    "                                  F.udf(f, VectorUDT())(dataset[self.getInputCol()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 120 µs, sys: 12 µs, total: 132 µs\n",
      "Wall time: 138 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# customized transformer class to manually extract some counting based word features\n",
    "class ReviewWordsTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=\"content\", outputCol=\"content_features\"):\n",
    "        super(ReviewWordsTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        def f(words):    \n",
    "            word_count = len(words)\n",
    "            unique_word_count = len(set(words))\n",
    "            upper_words = []\n",
    "            for w in words:\n",
    "                if w.isupper():\n",
    "                    upper_words.append(w)\n",
    "            upper_word_count = len(set(upper_words))\n",
    "            unique_upper_word_count = len(upper_words)\n",
    "            return Vectors.dense(word_count, unique_word_count, unique_word_count / (word_count + 1e-10),\n",
    "                                 upper_word_count, upper_word_count / (word_count + 1e-10), \n",
    "                                 unique_upper_word_count, unique_upper_word_count / (upper_word_count + 1e-10))\n",
    "\n",
    "        return dataset.withColumn(self.getOutputCol(), \n",
    "                                  F.udf(f, VectorUDT())(dataset[self.getInputCol()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 µs, sys: 0 ns, total: 9 µs\n",
      "Wall time: 16 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# show model prediction performance on the given dataset\n",
    "def eval_model_perf(fitted_model, dataset, label_col=\"label\", prediction_col=\"prediction\", probability_col=\"probability\"):\n",
    "    pred_dataset = fitted_model.transform(dataset)\n",
    "    eval_dataset = pred_dataset.select(label_col, prediction_col, probability_col)\n",
    "    # model performance evaluation\n",
    "    metricNames = [\"accuracy\", \"f1\"]\n",
    "    model_eval = MulticlassClassificationEvaluator(predictionCol=prediction_col, labelCol=label_col)\n",
    "    for m in metricNames:\n",
    "        val = model_eval.evaluate(eval_dataset, {model_eval.metricName: m})\n",
    "        print(m, \" = \", val)\n",
    "    roc_eval = BinaryClassificationEvaluator(rawPredictionCol=probability_col, labelCol=label_col, metricName=\"areaUnderROC\")\n",
    "    print(\"AUC =\", roc_eval.evaluate(eval_dataset))    \n",
    "    return pred_dataset\n",
    "\n",
    "# show CV param tunning result\n",
    "def show_cv_results(cv_model):\n",
    "    for result, param in sorted(zip(cv_model.avgMetrics, cv_model.getEstimatorParamMaps()), reverse=True, key=lambda x: x[0]):\n",
    "        print(result, \" | \", param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 11.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def build_data_preproc_model_with_pca(vocab_size=5000):\n",
    "    preproc_steps = [\n",
    "        RegexTokenizer(inputCol=\"content\", outputCol=\"all_words\", pattern=r\"\\W\"),\n",
    "        StopWordsRemover(inputCol=\"all_words\", outputCol=\"words\"),\n",
    "        CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=vocab_size),\n",
    "        IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\"),\n",
    "        PCA(inputCol=\"tfidf_features\", outputCol=\"pca_features\", k=100),\n",
    "        \n",
    "        ReviewContentTransformer(inputCol=\"content\", outputCol=\"content_features\"),\n",
    "        ReviewWordsTransformer(inputCol=\"words\", outputCol=\"word_features\"),\n",
    "        \n",
    "        RegexTokenizer(inputCol=\"title\", outputCol=\"all_title_words\", pattern=r\"\\W\"),\n",
    "        StopWordsRemover(inputCol=\"all_title_words\", outputCol=\"title_words\"),\n",
    "        CountVectorizer(inputCol=\"title_words\", outputCol=\"title_tf_features\", vocabSize=100),\n",
    "        IDF(inputCol=\"title_tf_features\", outputCol=\"title_tfidf_features\"),\n",
    "        PCA(inputCol=\"title_tfidf_features\", outputCol=\"title_pca_features\", k=100),   \n",
    "        \n",
    "        StringIndexer(inputCol=\"authors_missing\", outputCol=\"authors_missing_indexed\", handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol=\"authors_missing_indexed\", outputCol=\"authors_missing_feature\"),\n",
    "        \n",
    "        VectorAssembler(inputCols=[\"pca_features\", \"title_pca_features\", \n",
    "                                   \"content_features\", \"word_features\", \"authors_missing_feature\"], \n",
    "                        outputCol=\"features\")\n",
    "    ]\n",
    "    return Pipeline(stages=preproc_steps)\n",
    "\n",
    "def build_data_preproc_model_without_pca(vocab_size=5000):\n",
    "    preproc_steps = [\n",
    "        RegexTokenizer(inputCol=\"content\", outputCol=\"all_words\", pattern=r\"\\W\"),\n",
    "        StopWordsRemover(inputCol=\"all_words\", outputCol=\"words\"),\n",
    "        CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\", vocabSize=vocab_size),\n",
    "        IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\"),\n",
    "        \n",
    "        RegexTokenizer(inputCol=\"title\", outputCol=\"all_title_words\", pattern=r\"\\W\"),\n",
    "        StopWordsRemover(inputCol=\"all_title_words\", outputCol=\"title_words\"),\n",
    "        CountVectorizer(inputCol=\"title_words\", outputCol=\"title_tf_features\", vocabSize=100),\n",
    "        IDF(inputCol=\"title_tf_features\", outputCol=\"title_tfidf_features\"),\n",
    "        \n",
    "        StringIndexer(inputCol=\"authors_missing\", outputCol=\"authors_missing_indexed\", handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol=\"authors_missing_indexed\", outputCol=\"authors_missing_feature\"),\n",
    "        \n",
    "        ReviewContentTransformer(inputCol=\"content\", outputCol=\"content_features\"),\n",
    "        ReviewWordsTransformer(inputCol=\"words\", outputCol=\"word_features\"),\n",
    "        \n",
    "        VectorAssembler(inputCols=[\"tfidf_features\", \"title_tfidf_features\", \n",
    "                                   \"content_features\", \"word_features\", \"authors_missing_feature\"],\n",
    "                        outputCol=\"features\")\n",
    "    ]\n",
    "    return Pipeline(stages=preproc_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Run Models with PCA Features**********\n",
      "CPU times: user 686 ms, sys: 236 ms, total: 922 ms\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"**********Run Models with PCA Features**********\")\n",
    "# generate the features to be used for model training\n",
    "preproc_model = build_data_preproc_model_with_pca(2000).fit(df_train)\n",
    "df_train_pca = preproc_model.transform(df_train).select(\"label\", \"features\")\n",
    "df_test_pca = preproc_model.transform(df_test).select(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.8898226819357222\n",
      "f1  =  0.8897138135448337\n",
      "AUC = 0.9528308940343868\n",
      "CPU times: user 4 s, sys: 1.95 s, total: 5.95 s\n",
      "Wall time: 3min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr_model = LogisticRegression(featuresCol='features', \n",
    "                              labelCol='label', \n",
    "                              predictionCol='prediction', \n",
    "                              probabilityCol='probability', \n",
    "                              rawPredictionCol='rawPrediction',\n",
    "                              family='binomial', \n",
    "                              fitIntercept=True, \n",
    "                              threshold=0.5, \n",
    "                              standardization=False, \n",
    "                              maxIter=200, \n",
    "                              regParam=0.005, \n",
    "                              elasticNetParam=0, \n",
    "                              tol=1e-06, \n",
    "                              aggregationDepth=2)\n",
    "\n",
    "lr_model = lr_model.fit(df_train_pca)\n",
    "eval_model_perf(lr_model, df_test_pca)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.8279691540450683\n",
      "f1  =  0.8279438999270583\n",
      "AUC = 0.8989649256198621\n",
      "CPU times: user 655 ms, sys: 237 ms, total: 892 ms\n",
      "Wall time: 5min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dt_model = DecisionTreeClassifier(featuresCol='features', \n",
    "                                  labelCol='label', \n",
    "                                  predictionCol='prediction', \n",
    "                                  probabilityCol='probability', \n",
    "                                  rawPredictionCol='rawPrediction', \n",
    "                                  maxDepth=10, maxBins=32, \n",
    "                                  minInstancesPerNode=1, \n",
    "                                  minInfoGain=0.0, \n",
    "                                  maxMemoryInMB=2048, \n",
    "                                  cacheNodeIds=True, \n",
    "                                  checkpointInterval=10,\n",
    "                                  impurity='gini', \n",
    "                                  seed=666)\n",
    "\n",
    "dt_model = dt_model.fit(df_train_pca)\n",
    "eval_model_perf(dt_model, df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.868235131141485\n",
      "f1  =  0.8682193093793285\n",
      "AUC = 0.9429120190508549\n",
      "CPU times: user 755 ms, sys: 254 ms, total: 1.01 s\n",
      "Wall time: 8min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf_model = RandomForestClassifier(featuresCol='features', \n",
    "                                  labelCol='label', \n",
    "                                  predictionCol='prediction', \n",
    "                                  probabilityCol='probability', \n",
    "                                  rawPredictionCol='rawPrediction',\n",
    "                                  maxDepth=10, \n",
    "                                  maxBins=32, \n",
    "                                  minInstancesPerNode=1, \n",
    "                                  minInfoGain=0.0, \n",
    "                                  maxMemoryInMB=2048, \n",
    "                                  cacheNodeIds=True, \n",
    "                                  checkpointInterval=10, \n",
    "                                  impurity='gini', \n",
    "                                  numTrees=200, \n",
    "                                  featureSubsetStrategy='auto', \n",
    "                                  seed=666, \n",
    "                                  subsamplingRate=0.8)\n",
    "\n",
    "rf_model = rf_model.fit(df_train_pca)\n",
    "eval_model_perf(rf_model, df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.9045760990025858\n",
      "f1  =  0.9045747967490159\n",
      "AUC = 0.9684544232585017\n",
      "CPU times: user 40.4 s, sys: 17.1 s, total: 57.5 s\n",
      "Wall time: 40min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gbt_model = GBTClassifier(featuresCol='features', \n",
    "                         labelCol='label', \n",
    "                         maxIter=250)\n",
    "\n",
    "gbt_model = gbt_model.fit(df_train_pca)\n",
    "eval_model_perf(gbt_model, df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Run Models without PCA Features**********\n",
      "CPU times: user 482 ms, sys: 98.4 ms, total: 580 ms\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"**********Run Models without PCA Features**********\")\n",
    "# generate the features to be used for model training\n",
    "preproc_model = build_data_preproc_model_without_pca(3000).fit(df_train)\n",
    "df_train_wo_pca = preproc_model.transform(df_train).select(\"label\", \"features\")\n",
    "df_test_wo_pca = preproc_model.transform(df_test).select(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.9532000369412634\n",
      "f1  =  0.9531987326791072\n",
      "AUC = 0.9884873734361753\n",
      "CPU times: user 3.96 s, sys: 2.07 s, total: 6.02 s\n",
      "Wall time: 4min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr_model = LogisticRegression(featuresCol='features', \n",
    "                              labelCol='label', \n",
    "                              predictionCol='prediction', \n",
    "                              probabilityCol='probability', \n",
    "                              rawPredictionCol='rawPrediction',\n",
    "                              family='binomial', \n",
    "                              fitIntercept=True, \n",
    "                              threshold=0.5, \n",
    "                              standardization=False, \n",
    "                              maxIter=200, \n",
    "                              regParam=0.005, \n",
    "                              elasticNetParam=0, \n",
    "                              tol=1e-06, \n",
    "                              aggregationDepth=2)\n",
    "\n",
    "lr_model = lr_model.fit(df_train_wo_pca)    \n",
    "eval_model_perf(lr_model, df_test_wo_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.9307582194311046\n",
      "f1  =  0.9307362530544104\n",
      "AUC = 0.972280896261124\n",
      "CPU times: user 564 ms, sys: 239 ms, total: 804 ms\n",
      "Wall time: 3min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dt_model = DecisionTreeClassifier(featuresCol='features', \n",
    "                                  labelCol='label', \n",
    "                                  predictionCol='prediction', \n",
    "                                  probabilityCol='probability', \n",
    "                                  rawPredictionCol='rawPrediction', \n",
    "                                  maxDepth=10, maxBins=32, \n",
    "                                  minInstancesPerNode=1, \n",
    "                                  minInfoGain=0.0, \n",
    "                                  maxMemoryInMB=2048, \n",
    "                                  cacheNodeIds=True, \n",
    "                                  checkpointInterval=10,\n",
    "                                  impurity='gini', \n",
    "                                  seed=666)\n",
    "\n",
    "dt_model = dt_model.fit(df_train_wo_pca)\n",
    "eval_model_perf(dt_model, df_test_wo_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.9313354266715922\n",
      "f1  =  0.9313173358398661\n",
      "AUC = 0.9846590644542499\n",
      "CPU times: user 733 ms, sys: 189 ms, total: 921 ms\n",
      "Wall time: 8min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf_model = RandomForestClassifier(featuresCol='features', \n",
    "                                  labelCol='label', \n",
    "                                  predictionCol='prediction', \n",
    "                                  probabilityCol='probability', \n",
    "                                  rawPredictionCol='rawPrediction',\n",
    "                                  maxDepth=10, \n",
    "                                  maxBins=32, \n",
    "                                  minInstancesPerNode=1, \n",
    "                                  minInfoGain=0.0, \n",
    "                                  maxMemoryInMB=2048, \n",
    "                                  cacheNodeIds=True, \n",
    "                                  checkpointInterval=10, \n",
    "                                  impurity='gini', \n",
    "                                  numTrees=200, \n",
    "                                  featureSubsetStrategy='auto', \n",
    "                                  seed=666, \n",
    "                                  subsamplingRate=0.8)\n",
    "\n",
    "rf_model = rf_model.fit(df_train_wo_pca)\n",
    "eval_model_perf(rf_model, df_test_wo_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.9589951976357591\n",
      "f1  =  0.9589939700573036\n",
      "AUC = 0.9944275714534736\n",
      "CPU times: user 46 s, sys: 18.9 s, total: 1min 4s\n",
      "Wall time: 1h 39min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gbt_model = GBTClassifier(featuresCol='features', \n",
    "                         labelCol='label', \n",
    "                         maxIter=250)\n",
    "\n",
    "gbt_model = gbt_model.fit(df_train_wo_pca)\n",
    "eval_model_perf(gbt_model, df_test_wo_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  =  0.8692510158847433\n",
      "f1  =  0.8692045661241024\n",
      "AUC = 0.9086540366430549\n",
      "CPU times: user 274 ms, sys: 96.9 ms, total: 371 ms\n",
      "Wall time: 2min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nb_model = NaiveBayes(featuresCol='features', \n",
    "                              labelCol='label', \n",
    "                              predictionCol='prediction', \n",
    "                              probabilityCol='probability', \n",
    "                              rawPredictionCol='rawPrediction', \n",
    "                              smoothing=1, \n",
    "                              modelType='multinomial')\n",
    "\n",
    "nb_model = nb_model.fit(df_train_wo_pca)\n",
    "eval_model_perf(nb_model, df_test_wo_pca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}